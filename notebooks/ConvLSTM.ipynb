{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb94f3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/manoj/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/manoj/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_70315/1000158021.py\", line 187, in __getitem__\n    input_tensor = torch.tensor(input_tensor, dtype=torch.float32)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 287\u001b[39m\n\u001b[32m    285\u001b[39m epochs = \u001b[32m10\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     loss, acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device)\u001b[39m\n\u001b[32m    247\u001b[39m total_correct = \u001b[32m0\u001b[39m\n\u001b[32m    248\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mTypeError\u001b[39m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/manoj/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/manoj/Desktop/MH/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_70315/1000158021.py\", line 187, in __getitem__\n    input_tensor = torch.tensor(input_tensor, dtype=torch.float32)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from scipy.spatial import KDTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Utility functions for lat-lon conversion and feature loading ---\n",
    "def format_coord(val):\n",
    "    if val.is_integer():\n",
    "        return f\"{int(val)}.0\"\n",
    "    else:\n",
    "        return f\"{val:.2f}\"\n",
    "\n",
    "def ddmm_to_decimal(ddmm):\n",
    "    degrees = int(ddmm) // 100\n",
    "    minutes = ddmm - (degrees * 100)\n",
    "    return round(degrees + minutes / 60, 4)\n",
    "\n",
    "def load_feature(lat, lon, base_dir='output_features'):\n",
    "    filename = f'lat_{format_coord(lat)}_lon_{format_coord(lon)}.csv'\n",
    "\n",
    "    path = os.path.join(base_dir, filename)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    df = pd.read_csv(path, parse_dates=['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    # Drop the date column, keep all feature columns\n",
    "    return df  # return entire dataframe with all feature columns except date\n",
    "\n",
    "\n",
    "\n",
    "def parse_coord(filename):\n",
    "    \"\"\"\n",
    "    Extract lat and lon from filename like 'lat_14.0_lon_71.25.csv'\n",
    "    \"\"\"\n",
    "    m = re.match(r'lat_([-+]?\\d*\\.?\\d+)_lon_([-+]?\\d*\\.?\\d+)\\.csv', filename)\n",
    "    if m:\n",
    "        lat = float(m.group(1))\n",
    "        lon = float(m.group(2))\n",
    "        return lat, lon\n",
    "    return None\n",
    "\n",
    "def get_available_coords(feature_dir='output_features'):\n",
    "    \"\"\"\n",
    "    Scan feature_dir and return list of (lat, lon) tuples from filenames\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for f in os.listdir(feature_dir):\n",
    "        coord = parse_coord(f)\n",
    "        if coord is not None:\n",
    "            coords.append(coord)\n",
    "    return coords\n",
    "\n",
    "def find_nearest_coord(lat, lon, kdtree, coord_array):\n",
    "    \"\"\"\n",
    "    Find nearest available coordinate from kdtree given (lat, lon)\n",
    "    \"\"\"\n",
    "    dist, idx = kdtree.query([lat, lon])\n",
    "    return tuple(coord_array[idx])\n",
    "\n",
    "def build_spatiotemporal_tensor(station_lat, station_lon, dates, grid_size=5, feature_dir='output_features'):\n",
    "    \"\"\"\n",
    "    Build spatiotemporal tensor for given station lat/lon and list of dates.\n",
    "    Finds nearest available feature files around the station within grid_size.\n",
    "    \n",
    "    Params:\n",
    "    - station_lat, station_lon: float, station coordinates\n",
    "    - dates: list or pd.DatetimeIndex of dates to extract features for\n",
    "    - grid_size: int, must be odd, grid width around station (e.g. 5 for 5x5)\n",
    "    - feature_dir: str, directory containing feature CSV files named lat_x_lon_y.csv\n",
    "    \n",
    "    Returns:\n",
    "    - tensor: np.array of shape (len(dates), grid_size*grid_size, num_features)\n",
    "    \"\"\"\n",
    "    offset = grid_size // 2\n",
    "    desired_grid = [(station_lat + 0.25 * i, station_lon + 0.25 * j)\n",
    "                    for i in range(-offset, offset + 1)\n",
    "                    for j in range(-offset, offset + 1)]\n",
    "\n",
    "    available_coords = get_available_coords(feature_dir)\n",
    "    if len(available_coords) == 0:\n",
    "        raise ValueError(f\"No feature files found in {feature_dir}\")\n",
    "\n",
    "    coord_array = np.array(available_coords)\n",
    "    kdtree = KDTree(coord_array)\n",
    "\n",
    "    tensor_slices = []\n",
    "    used_coords = set()\n",
    "\n",
    "    for lat, lon in desired_grid:\n",
    "        nearest_coord = find_nearest_coord(lat, lon, kdtree, coord_array)\n",
    "        if nearest_coord in used_coords:\n",
    "            # Avoid duplicate grid cells if nearest neighbors overlap\n",
    "            continue\n",
    "        used_coords.add(nearest_coord)\n",
    "\n",
    "        filename = f'lat_{nearest_coord[0]}_lon_{nearest_coord[1]}.csv'\n",
    "        filepath = os.path.join(feature_dir, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            # Should not happen since we got coords from files\n",
    "            raise ValueError(f\"Feature file missing: {filepath}\")\n",
    "\n",
    "        # Read features for the dates\n",
    "        df = pd.read_csv(filepath, parse_dates=['date']).set_index('date')\n",
    "\n",
    "        # Ensure all dates present, else you can decide to handle missing ones (e.g. fillna)\n",
    "        # Select rows for requested dates\n",
    "        df_sub = df.loc[df.index.isin(dates)]\n",
    "\n",
    "        if df_sub.empty:\n",
    "            raise ValueError(f\"No feature data found for dates in {filename}\")\n",
    "\n",
    "        # Drop 'date' column if still present; all other columns are features\n",
    "        # (We set date as index, so no need)\n",
    "        features = df_sub.values  # shape (num_dates, num_features)\n",
    "\n",
    "        tensor_slices.append(features)\n",
    "\n",
    "    # Now tensor_slices is list of arrays (num_dates x num_features) for each grid cell\n",
    "    # Stack along second dimension: result shape (num_dates, num_grid_cells, num_features)\n",
    "    tensor = np.stack(tensor_slices, axis=1)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def load_and_binarize_labels(station_file, label_dir='per_station_data', percentile=0.85):\n",
    "    label_path = os.path.join(label_dir, station_file)\n",
    "    labels_df = pd.read_csv(label_path, parse_dates=['Date'])\n",
    "    labels_df.set_index('Date', inplace=True)\n",
    "\n",
    "    threshold = labels_df['Rainfall'].quantile(percentile)\n",
    "    labels_df['rainfall_binary'] = (labels_df['Rainfall'] >= threshold).astype(int)\n",
    "    return labels_df\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "\n",
    "class RainfallDataset(Dataset):\n",
    "    def __init__(self, station_files, label_dir='per_station_data', feature_dir='output_features', \n",
    "                 T=7, grid_size=5, percentile=0.85):\n",
    "        self.station_files = station_files\n",
    "        self.label_dir = label_dir\n",
    "        self.feature_dir = feature_dir\n",
    "        self.T = T\n",
    "        self.grid_size = grid_size\n",
    "        self.percentile = percentile\n",
    "\n",
    "        # Preload labels with binary classification for each station\n",
    "        self.labels_dict = {}\n",
    "        for sf in station_files:\n",
    "            self.labels_dict[sf] = load_and_binarize_labels(sf, label_dir, percentile)\n",
    "\n",
    "        # Build list of (station_file, target_date) pairs for dataset indexing\n",
    "        self.samples = []\n",
    "        for sf in station_files:\n",
    "            label_dates = self.labels_dict[sf].index\n",
    "            for dt in label_dates:\n",
    "                # skip dates that don't have enough history for T days\n",
    "                if dt - timedelta(days=T-1) < label_dates.min():\n",
    "                    continue\n",
    "                self.samples.append((sf, dt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        station_file, target_date = self.samples[idx]\n",
    "\n",
    "        # Parse lat lon from filename DDMM.M format\n",
    "        lat_ddmm = float(station_file.split('_')[1])\n",
    "        lon_ddmm = float(station_file.split('_')[2].replace('.csv', ''))\n",
    "\n",
    "        station_lat = ddmm_to_decimal(lat_ddmm)\n",
    "        station_lon = ddmm_to_decimal(lon_ddmm)\n",
    "\n",
    "        # Build dates for input features\n",
    "        input_dates = [target_date - timedelta(days=i) for i in reversed(range(self.T))]\n",
    "\n",
    "        # Build input tensor (T, C, H, W)\n",
    "        input_tensor = build_spatiotemporal_tensor(station_lat, station_lon, input_dates,\n",
    "                                                   grid_size=self.grid_size, feature_dir=self.feature_dir)\n",
    "        input_tensor = torch.tensor(input_tensor, dtype=torch.float32)\n",
    "\n",
    "        # Get binary label for target date\n",
    "        label = self.labels_dict[station_file].loc[target_date, 'rainfall_binary']\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return input_tensor, label\n",
    "\n",
    "# --- ConvLSTM model classes (same as before) ---\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_channels + hidden_channels, 4 * hidden_channels, kernel_size, padding=padding)\n",
    "\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)  # concat on channels\n",
    "        conv_out = self.conv(combined)\n",
    "        (cc_i, cc_f, cc_o, cc_g) = torch.split(conv_out, self.hidden_channels, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_channels=16, kernel_size=3, num_layers=1, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # Pool spatial dims to 1x1\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, time, channels, height, width)\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        h = torch.zeros(batch_size, self.hidden_channels, H, W, device=x.device)\n",
    "        c = torch.zeros(batch_size, self.hidden_channels, H, W, device=x.device)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.cell(x[:, t], h, c)\n",
    "\n",
    "        out = self.pool(h).view(batch_size, -1)\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out).squeeze(1)  # sigmoid for binary\n",
    "        return out\n",
    "\n",
    "# --- Training loop ---\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_samples += X.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# --- Putting it all together ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # List all station label files you want to train on\n",
    "    station_files = ['DHARMABAD_1853.0_7750.0.csv']\n",
    "  # or list manually\n",
    "\n",
    "    # Create Dataset and DataLoader\n",
    "    dataset = RainfallDataset(station_files)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Create model, optimizer, loss\n",
    "    model = ConvLSTM().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        loss, acc = train(model, dataloader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
