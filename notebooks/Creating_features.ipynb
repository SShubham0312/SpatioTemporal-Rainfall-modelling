{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bccf28f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71546fe9",
   "metadata": {},
   "source": [
    "### Exploring GFS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba81b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing the 5 folders\n",
    "base_dir = \"GFS_Data_6to48\"\n",
    "subfolders = [\"Prate\", \"Pwater\", \"Pressure\", \"Temperature\", \"RH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf73596",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'GFS_Data_6to48\\\\Prate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, subfolder)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Pick the first NetCDF file in the folder\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ No NetCDF files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'GFS_Data_6to48\\\\Prate'"
     ]
    }
   ],
   "source": [
    "for subfolder in subfolders:\n",
    "    folder_path = os.path.join(base_dir, subfolder)\n",
    "    \n",
    "    # Pick the first NetCDF file in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".nc\")]\n",
    "    if not files:\n",
    "        print(f\"âŒ No NetCDF files found in {subfolder}\")\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(folder_path, files[0])\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(f\"\\nðŸ” Exploring: {file_path}\")\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Show basic info\n",
    "    print(ds)\n",
    "    \n",
    "    # Print variables\n",
    "    print(\"\\nðŸ“¦ Variables:\")\n",
    "    for var in ds.data_vars:\n",
    "        print(f\" - {var}\")\n",
    "    \n",
    "    # Print dimensions\n",
    "    print(\"\\nðŸ“ Dimensions:\")\n",
    "    for dim in ds.dims:\n",
    "        print(f\" - {dim}: {ds.dims[dim]}\")\n",
    "    \n",
    "    # Print coordinate ranges\n",
    "    print(\"\\nðŸ—ºï¸ Coordinates:\")\n",
    "    for coord in ds.coords:\n",
    "        print(f\" - {coord}: {ds[coord].values[:3]} ... {ds[coord].values[-3:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449641d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         init_date, init_hour, fhr \u001b[38;5;241m=\u001b[39m \u001b[43mparse_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m         file_index[var][(init_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), init_hour)][fhr] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, fname)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Extract lat/lon grid from any one file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m, in \u001b[0;36mparse_filename\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     20\u001b[0m datetime_part \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# '20190101_00_f018'\u001b[39;00m\n\u001b[0;32m     21\u001b[0m dt_str \u001b[38;5;241m=\u001b[39m datetime_part\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m date_str, hour_str, fhr_str \u001b[38;5;241m=\u001b[39m dt_str[\u001b[38;5;241m0\u001b[39m], \u001b[43mdt_str\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, dt_str[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     23\u001b[0m init_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(date_str, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m init_hour \u001b[38;5;241m=\u001b[39m hour_str\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "VARIABLES = ['R_H_L100', 'TMP_L100', 'PRES_L1', 'P_WAT_L200', 'PRATE_L1']\n",
    "INIT_TIMES = ['00', '06', '12', '18']\n",
    "FHR_DICT = {\n",
    "    '00': range(18, 43, 3),  # 18 to 42\n",
    "    '06': range(12, 37, 3),\n",
    "    '12': range(6, 31, 3),\n",
    "    '18': range(24, 49, 3),\n",
    "}\n",
    "\n",
    "# Folder where NetCDF files are stored\n",
    "BASE_DIR = \"GFS_Data_6to48\"  # Update to actual path\n",
    "OUTPUT_DIR = \"output_features\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_filename(filename):\n",
    "    # Example: gfs.0p25.20190101_00_f018.grib2.nc\n",
    "    parts = filename.split('.')\n",
    "    datetime_part = parts[2]  # '20190101_00_f018'\n",
    "    dt_str = datetime_part.split('_')\n",
    "    date_str, hour_str, fhr_str = dt_str[0], dt_str[1], dt_str[2]\n",
    "    init_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    init_hour = hour_str\n",
    "    fhr = int(fhr_str[1:])\n",
    "    return init_date, init_hour, fhr\n",
    "\n",
    "\n",
    "# Build file index\n",
    "file_index = {var: defaultdict(dict) for var in VARIABLES}\n",
    "\n",
    "for var in VARIABLES:\n",
    "    folder = os.path.join(BASE_DIR, var)\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.endswith(\".nc\"):\n",
    "            continue\n",
    "        init_date, init_hour, fhr = parse_filename(fname)\n",
    "        file_index[var][(init_date.strftime(\"%Y-%m-%d\"), init_hour)][fhr] = os.path.join(folder, fname)\n",
    "\n",
    "# Extract lat/lon grid from any one file\n",
    "sample_file = next(iter(file_index['TMP_L100'].values()))[18]\n",
    "ds_sample = xr.open_dataset(sample_file)\n",
    "lats = ds_sample['latitude'].values\n",
    "lons = ds_sample['longitude'].values\n",
    "lat_lon_pairs = [(float(lat), float(lon)) for lat in lats for lon in lons]\n",
    "\n",
    "# Initialize data holders\n",
    "data_by_location = {loc: [] for loc in lat_lon_pairs}\n",
    "\n",
    "# Define date range (update as needed)\n",
    "start_date = datetime(2019, 1, 1)\n",
    "end_date = datetime(2019, 1, 3)\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    utc_day = current_date - timedelta(hours=5, minutes=30)\n",
    "    rows_by_loc = {loc: {'date': current_date.strftime(\"%Y-%m-%d\")} for loc in lat_lon_pairs}\n",
    "\n",
    "    valid_day = True\n",
    "\n",
    "    for init_time in INIT_TIMES:\n",
    "        if init_time == '18':\n",
    "            init_date = utc_day.date() - timedelta(days=2)\n",
    "        else:\n",
    "            init_date = utc_day.date() - timedelta(days=1)\n",
    "\n",
    "        for var in VARIABLES:\n",
    "            for fhr in FHR_DICT[init_time]:\n",
    "                init_key = (init_date.strftime(\"%Y-%m-%d\"), init_time)\n",
    "                filepath = file_index.get(var, {}).get(init_key, {}).get(fhr)\n",
    "\n",
    "                if not filepath or not os.path.exists(filepath):\n",
    "                    valid_day = False\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    ds = xr.open_dataset(filepath)\n",
    "                    data_array = ds[var].isel(time=0).values  # 2D array\n",
    "                    for i, lat in enumerate(lats):\n",
    "                        for j, lon in enumerate(lons):\n",
    "                            key = (float(lat), float(lon))\n",
    "                            colname = f\"{var}_{init_time}_f{fhr:03d}\"\n",
    "                            rows_by_loc[key][colname] = float(data_array[i, j])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {filepath}: {e}\")\n",
    "                    valid_day = False\n",
    "\n",
    "    if valid_day:\n",
    "        for loc, row in rows_by_loc.items():\n",
    "            data_by_location[loc].append(row)\n",
    "\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Save CSVs\n",
    "for loc, records in data_by_location.items():\n",
    "    lat, lon = loc\n",
    "    df = pd.DataFrame(records)\n",
    "    filename = f\"lat_{lat:.2f}_lon_{lon:.2f}.csv\"\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, filename), index=False)\n",
    "\n",
    "\"Code completed. Sample CSVs will be saved in output_features directory.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b64695",
   "metadata": {},
   "source": [
    "#### Converting data into date-feature data for individual lat-long pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153811e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing variables: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.52it/s]\n",
      "Processing dates:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1369/2100 [4:26:35<2:12:16, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20230328 18 | f042\n",
      "Missing: TMP_L100 | 20230328 18 | f042\n",
      "Missing: PRES_L1 | 20230328 18 | f042\n",
      "Missing: P_WAT_L200 | 20230328 18 | f042\n",
      "Missing: PRATE_L1 | 20230328 18 | f042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1641/2100 [5:17:22<1:36:55, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20231225 18 | f024\n",
      "Missing: R_H_L100 | 20231225 18 | f027\n",
      "Missing: R_H_L100 | 20231225 18 | f030\n",
      "Missing: R_H_L100 | 20231225 18 | f033\n",
      "Missing: R_H_L100 | 20231225 18 | f036\n",
      "Missing: R_H_L100 | 20231225 18 | f039\n",
      "Missing: R_H_L100 | 20231225 18 | f042\n",
      "Missing: R_H_L100 | 20231225 18 | f045\n",
      "Missing: R_H_L100 | 20231225 18 | f048\n",
      "Missing: TMP_L100 | 20231225 18 | f024\n",
      "Missing: TMP_L100 | 20231225 18 | f027\n",
      "Missing: TMP_L100 | 20231225 18 | f030\n",
      "Missing: TMP_L100 | 20231225 18 | f033\n",
      "Missing: TMP_L100 | 20231225 18 | f036\n",
      "Missing: TMP_L100 | 20231225 18 | f039\n",
      "Missing: TMP_L100 | 20231225 18 | f042\n",
      "Missing: TMP_L100 | 20231225 18 | f045\n",
      "Missing: TMP_L100 | 20231225 18 | f048\n",
      "Missing: PRES_L1 | 20231225 18 | f024\n",
      "Missing: PRES_L1 | 20231225 18 | f027\n",
      "Missing: PRES_L1 | 20231225 18 | f030\n",
      "Missing: PRES_L1 | 20231225 18 | f033\n",
      "Missing: PRES_L1 | 20231225 18 | f036\n",
      "Missing: PRES_L1 | 20231225 18 | f039\n",
      "Missing: PRES_L1 | 20231225 18 | f042\n",
      "Missing: PRES_L1 | 20231225 18 | f045\n",
      "Missing: PRES_L1 | 20231225 18 | f048\n",
      "Missing: P_WAT_L200 | 20231225 18 | f024\n",
      "Missing: P_WAT_L200 | 20231225 18 | f027\n",
      "Missing: P_WAT_L200 | 20231225 18 | f030\n",
      "Missing: P_WAT_L200 | 20231225 18 | f033\n",
      "Missing: P_WAT_L200 | 20231225 18 | f036\n",
      "Missing: P_WAT_L200 | 20231225 18 | f039\n",
      "Missing: P_WAT_L200 | 20231225 18 | f042\n",
      "Missing: P_WAT_L200 | 20231225 18 | f045\n",
      "Missing: P_WAT_L200 | 20231225 18 | f048\n",
      "Missing: PRATE_L1 | 20231225 18 | f024\n",
      "Missing: PRATE_L1 | 20231225 18 | f027\n",
      "Missing: PRATE_L1 | 20231225 18 | f030\n",
      "Missing: PRATE_L1 | 20231225 18 | f033\n",
      "Missing: PRATE_L1 | 20231225 18 | f036\n",
      "Missing: PRATE_L1 | 20231225 18 | f039\n",
      "Missing: PRATE_L1 | 20231225 18 | f042\n",
      "Missing: PRATE_L1 | 20231225 18 | f045\n",
      "Missing: PRATE_L1 | 20231225 18 | f048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1787/2100 [5:43:30<53:50, 10.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20240519 18 | f024\n",
      "Missing: R_H_L100 | 20240519 18 | f027\n",
      "Missing: R_H_L100 | 20240519 18 | f030\n",
      "Missing: R_H_L100 | 20240519 18 | f033\n",
      "Missing: R_H_L100 | 20240519 18 | f036\n",
      "Missing: R_H_L100 | 20240519 18 | f039\n",
      "Missing: R_H_L100 | 20240519 18 | f042\n",
      "Missing: R_H_L100 | 20240519 18 | f045\n",
      "Missing: R_H_L100 | 20240519 18 | f048\n",
      "Missing: TMP_L100 | 20240519 18 | f024\n",
      "Missing: TMP_L100 | 20240519 18 | f027\n",
      "Missing: TMP_L100 | 20240519 18 | f030\n",
      "Missing: TMP_L100 | 20240519 18 | f033\n",
      "Missing: TMP_L100 | 20240519 18 | f036\n",
      "Missing: TMP_L100 | 20240519 18 | f039\n",
      "Missing: TMP_L100 | 20240519 18 | f042\n",
      "Missing: TMP_L100 | 20240519 18 | f045\n",
      "Missing: TMP_L100 | 20240519 18 | f048\n",
      "Missing: PRES_L1 | 20240519 18 | f024\n",
      "Missing: PRES_L1 | 20240519 18 | f027\n",
      "Missing: PRES_L1 | 20240519 18 | f030\n",
      "Missing: PRES_L1 | 20240519 18 | f033\n",
      "Missing: PRES_L1 | 20240519 18 | f036\n",
      "Missing: PRES_L1 | 20240519 18 | f039\n",
      "Missing: PRES_L1 | 20240519 18 | f042\n",
      "Missing: PRES_L1 | 20240519 18 | f045\n",
      "Missing: PRES_L1 | 20240519 18 | f048\n",
      "Missing: P_WAT_L200 | 20240519 18 | f024\n",
      "Missing: P_WAT_L200 | 20240519 18 | f027\n",
      "Missing: P_WAT_L200 | 20240519 18 | f030\n",
      "Missing: P_WAT_L200 | 20240519 18 | f033\n",
      "Missing: P_WAT_L200 | 20240519 18 | f036\n",
      "Missing: P_WAT_L200 | 20240519 18 | f039\n",
      "Missing: P_WAT_L200 | 20240519 18 | f042\n",
      "Missing: P_WAT_L200 | 20240519 18 | f045\n",
      "Missing: P_WAT_L200 | 20240519 18 | f048\n",
      "Missing: PRATE_L1 | 20240519 18 | f024\n",
      "Missing: PRATE_L1 | 20240519 18 | f027\n",
      "Missing: PRATE_L1 | 20240519 18 | f030\n",
      "Missing: PRATE_L1 | 20240519 18 | f033\n",
      "Missing: PRATE_L1 | 20240519 18 | f036\n",
      "Missing: PRATE_L1 | 20240519 18 | f039\n",
      "Missing: PRATE_L1 | 20240519 18 | f042\n",
      "Missing: PRATE_L1 | 20240519 18 | f045\n",
      "Missing: PRATE_L1 | 20240519 18 | f048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1795/2100 [5:44:52<51:54, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20240528 06 | f015\n",
      "Missing: R_H_L100 | 20240528 06 | f018\n",
      "Missing: R_H_L100 | 20240528 06 | f021\n",
      "Missing: R_H_L100 | 20240528 06 | f024\n",
      "Missing: R_H_L100 | 20240528 06 | f027\n",
      "Missing: R_H_L100 | 20240528 06 | f030\n",
      "Missing: R_H_L100 | 20240528 06 | f033\n",
      "Missing: R_H_L100 | 20240528 06 | f036\n",
      "Missing: TMP_L100 | 20240528 06 | f015\n",
      "Missing: TMP_L100 | 20240528 06 | f018\n",
      "Missing: TMP_L100 | 20240528 06 | f021\n",
      "Missing: TMP_L100 | 20240528 06 | f024\n",
      "Missing: TMP_L100 | 20240528 06 | f027\n",
      "Missing: TMP_L100 | 20240528 06 | f030\n",
      "Missing: TMP_L100 | 20240528 06 | f033\n",
      "Missing: TMP_L100 | 20240528 06 | f036\n",
      "Missing: PRES_L1 | 20240528 06 | f015\n",
      "Missing: PRES_L1 | 20240528 06 | f018\n",
      "Missing: PRES_L1 | 20240528 06 | f021\n",
      "Missing: PRES_L1 | 20240528 06 | f024\n",
      "Missing: PRES_L1 | 20240528 06 | f027\n",
      "Missing: PRES_L1 | 20240528 06 | f030\n",
      "Missing: PRES_L1 | 20240528 06 | f033\n",
      "Missing: PRES_L1 | 20240528 06 | f036\n",
      "Missing: P_WAT_L200 | 20240528 06 | f015\n",
      "Missing: P_WAT_L200 | 20240528 06 | f018\n",
      "Missing: P_WAT_L200 | 20240528 06 | f021\n",
      "Missing: P_WAT_L200 | 20240528 06 | f024\n",
      "Missing: P_WAT_L200 | 20240528 06 | f027\n",
      "Missing: P_WAT_L200 | 20240528 06 | f030\n",
      "Missing: P_WAT_L200 | 20240528 06 | f033\n",
      "Missing: P_WAT_L200 | 20240528 06 | f036\n",
      "Missing: PRATE_L1 | 20240528 06 | f015\n",
      "Missing: PRATE_L1 | 20240528 06 | f018\n",
      "Missing: PRATE_L1 | 20240528 06 | f021\n",
      "Missing: PRATE_L1 | 20240528 06 | f024\n",
      "Missing: PRATE_L1 | 20240528 06 | f027\n",
      "Missing: PRATE_L1 | 20240528 06 | f030\n",
      "Missing: PRATE_L1 | 20240528 06 | f033\n",
      "Missing: PRATE_L1 | 20240528 06 | f036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1905/2100 [6:23:38<1:30:43, 27.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20240915 00 | f039\n",
      "Missing: PRES_L1 | 20240915 00 | f039\n",
      "Missing: P_WAT_L200 | 20240915 00 | f039\n",
      "Missing: PRATE_L1 | 20240915 00 | f039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1941/2100 [6:40:11<1:11:52, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20241021 12 | f006\n",
      "Missing: R_H_L100 | 20241021 12 | f009\n",
      "Missing: R_H_L100 | 20241021 12 | f012\n",
      "Missing: R_H_L100 | 20241021 12 | f015\n",
      "Missing: R_H_L100 | 20241021 12 | f018\n",
      "Missing: R_H_L100 | 20241021 12 | f021\n",
      "Missing: R_H_L100 | 20241021 12 | f024\n",
      "Missing: R_H_L100 | 20241021 12 | f027\n",
      "Missing: R_H_L100 | 20241021 12 | f030\n",
      "Missing: TMP_L100 | 20241021 12 | f006\n",
      "Missing: TMP_L100 | 20241021 12 | f009\n",
      "Missing: TMP_L100 | 20241021 12 | f012\n",
      "Missing: TMP_L100 | 20241021 12 | f015\n",
      "Missing: TMP_L100 | 20241021 12 | f018\n",
      "Missing: TMP_L100 | 20241021 12 | f021\n",
      "Missing: TMP_L100 | 20241021 12 | f024\n",
      "Missing: TMP_L100 | 20241021 12 | f027\n",
      "Missing: TMP_L100 | 20241021 12 | f030\n",
      "Missing: PRES_L1 | 20241021 12 | f006\n",
      "Missing: PRES_L1 | 20241021 12 | f009\n",
      "Missing: PRES_L1 | 20241021 12 | f012\n",
      "Missing: PRES_L1 | 20241021 12 | f015\n",
      "Missing: PRES_L1 | 20241021 12 | f018\n",
      "Missing: PRES_L1 | 20241021 12 | f021\n",
      "Missing: PRES_L1 | 20241021 12 | f024\n",
      "Missing: PRES_L1 | 20241021 12 | f027\n",
      "Missing: PRES_L1 | 20241021 12 | f030\n",
      "Missing: P_WAT_L200 | 20241021 12 | f006\n",
      "Missing: P_WAT_L200 | 20241021 12 | f009\n",
      "Missing: P_WAT_L200 | 20241021 12 | f012\n",
      "Missing: P_WAT_L200 | 20241021 12 | f015\n",
      "Missing: P_WAT_L200 | 20241021 12 | f018\n",
      "Missing: P_WAT_L200 | 20241021 12 | f021\n",
      "Missing: P_WAT_L200 | 20241021 12 | f024\n",
      "Missing: P_WAT_L200 | 20241021 12 | f027\n",
      "Missing: P_WAT_L200 | 20241021 12 | f030\n",
      "Missing: PRATE_L1 | 20241021 12 | f006\n",
      "Missing: PRATE_L1 | 20241021 12 | f009\n",
      "Missing: PRATE_L1 | 20241021 12 | f012\n",
      "Missing: PRATE_L1 | 20241021 12 | f015\n",
      "Missing: PRATE_L1 | 20241021 12 | f018\n",
      "Missing: PRATE_L1 | 20241021 12 | f021\n",
      "Missing: PRATE_L1 | 20241021 12 | f024\n",
      "Missing: PRATE_L1 | 20241021 12 | f027\n",
      "Missing: PRATE_L1 | 20241021 12 | f030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1957/2100 [6:47:26<1:05:44, 27.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: R_H_L100 | 20241106 06 | f030\n",
      "Missing: R_H_L100 | 20241106 06 | f033\n",
      "Missing: R_H_L100 | 20241106 06 | f036\n",
      "Missing: TMP_L100 | 20241106 06 | f030\n",
      "Missing: TMP_L100 | 20241106 06 | f033\n",
      "Missing: TMP_L100 | 20241106 06 | f036\n",
      "Missing: PRES_L1 | 20241106 06 | f030\n",
      "Missing: PRES_L1 | 20241106 06 | f033\n",
      "Missing: PRES_L1 | 20241106 06 | f036\n",
      "Missing: P_WAT_L200 | 20241106 06 | f030\n",
      "Missing: P_WAT_L200 | 20241106 06 | f033\n",
      "Missing: P_WAT_L200 | 20241106 06 | f036\n",
      "Missing: PRATE_L1 | 20241106 06 | f030\n",
      "Missing: PRATE_L1 | 20241106 06 | f033\n",
      "Missing: PRATE_L1 | 20241106 06 | f036\n",
      "Missing: R_H_L100 | 20241106 12 | f006\n",
      "Missing: R_H_L100 | 20241106 12 | f009\n",
      "Missing: R_H_L100 | 20241106 12 | f012\n",
      "Missing: R_H_L100 | 20241106 12 | f015\n",
      "Missing: R_H_L100 | 20241106 12 | f018\n",
      "Missing: R_H_L100 | 20241106 12 | f021\n",
      "Missing: R_H_L100 | 20241106 12 | f024\n",
      "Missing: R_H_L100 | 20241106 12 | f027\n",
      "Missing: R_H_L100 | 20241106 12 | f030\n",
      "Missing: TMP_L100 | 20241106 12 | f006\n",
      "Missing: TMP_L100 | 20241106 12 | f009\n",
      "Missing: TMP_L100 | 20241106 12 | f012\n",
      "Missing: TMP_L100 | 20241106 12 | f015\n",
      "Missing: TMP_L100 | 20241106 12 | f018\n",
      "Missing: TMP_L100 | 20241106 12 | f021\n",
      "Missing: TMP_L100 | 20241106 12 | f024\n",
      "Missing: TMP_L100 | 20241106 12 | f027\n",
      "Missing: TMP_L100 | 20241106 12 | f030\n",
      "Missing: PRES_L1 | 20241106 12 | f006\n",
      "Missing: PRES_L1 | 20241106 12 | f009\n",
      "Missing: PRES_L1 | 20241106 12 | f012\n",
      "Missing: PRES_L1 | 20241106 12 | f015\n",
      "Missing: PRES_L1 | 20241106 12 | f018\n",
      "Missing: PRES_L1 | 20241106 12 | f021\n",
      "Missing: PRES_L1 | 20241106 12 | f024\n",
      "Missing: PRES_L1 | 20241106 12 | f027\n",
      "Missing: PRES_L1 | 20241106 12 | f030\n",
      "Missing: P_WAT_L200 | 20241106 12 | f006\n",
      "Missing: P_WAT_L200 | 20241106 12 | f009\n",
      "Missing: P_WAT_L200 | 20241106 12 | f012\n",
      "Missing: P_WAT_L200 | 20241106 12 | f015\n",
      "Missing: P_WAT_L200 | 20241106 12 | f018\n",
      "Missing: P_WAT_L200 | 20241106 12 | f021\n",
      "Missing: P_WAT_L200 | 20241106 12 | f024\n",
      "Missing: P_WAT_L200 | 20241106 12 | f027\n",
      "Missing: P_WAT_L200 | 20241106 12 | f030\n",
      "Missing: PRATE_L1 | 20241106 12 | f006\n",
      "Missing: PRATE_L1 | 20241106 12 | f009\n",
      "Missing: PRATE_L1 | 20241106 12 | f012\n",
      "Missing: PRATE_L1 | 20241106 12 | f015\n",
      "Missing: PRATE_L1 | 20241106 12 | f018\n",
      "Missing: PRATE_L1 | 20241106 12 | f021\n",
      "Missing: PRATE_L1 | 20241106 12 | f024\n",
      "Missing: PRATE_L1 | 20241106 12 | f027\n",
      "Missing: PRATE_L1 | 20241106 12 | f030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2100/2100 [7:52:18<00:00, 13.49s/it]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from netCDF4 import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------ Step 0: Setup ------------------------\n",
    "variables = ['R_H_L100', 'TMP_L100', 'PRES_L1', 'P_WAT_L200', 'PRATE_L1']\n",
    "init_times = ['00', '06', '12', '18']\n",
    "forecast_hours_needed = {\n",
    "    '00': range(18, 43, 3),\n",
    "    '06': range(12, 37, 3),\n",
    "    '12': range(6, 31, 3),\n",
    "    '18': range(24, 49, 3),\n",
    "}\n",
    "\n",
    "data_root = 'GFS_Data_6to48'\n",
    "output_dir = 'output_features'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_file = Path(\"last_processed_date.txt\")\n",
    "\n",
    "# ------------------------ Step 1: Parse filename ------------------------\n",
    "def parse_filename(filename):\n",
    "    # gfs.0p25.2019011712.f030.grib2.nc\n",
    "    parts = filename.split('.')\n",
    "    init_str = parts[2]  # '2019011712'\n",
    "    fhr = int(parts[3][1:])  # 'f030' -> 30\n",
    "    init_date = init_str[:8]  # '20190117'\n",
    "    init_hour = init_str[8:]  # '12'\n",
    "    return init_date, init_hour, fhr\n",
    "\n",
    "# ------------------------ Step 2: Build file index ------------------------\n",
    "file_index = {var: defaultdict(dict) for var in variables}\n",
    "\n",
    "for var in tqdm(variables, desc=\"Indexing variables\"):\n",
    "    var_dir = os.path.join(data_root, var)\n",
    "    for file in os.listdir(var_dir):\n",
    "        if not file.endswith('.nc'):\n",
    "            continue\n",
    "        init_date, init_hour, fhr = parse_filename(file)\n",
    "        full_path = os.path.join(var_dir, file)\n",
    "        file_index[var][(init_date, init_hour)][fhr] = full_path\n",
    "\n",
    "# ------------------------ Step 3: Get lat/lon grid ------------------------\n",
    "sample_path = next(iter(next(iter(file_index[variables[0]].values())).values()))\n",
    "with Dataset(sample_path, 'r') as ds:\n",
    "    lats = ds.variables['lat'][:]\n",
    "    lons = ds.variables['lon'][:]\n",
    "\n",
    "lat_lon_grid = [(float(lat), float(lon)) for lat in lats for lon in lons]\n",
    "\n",
    "# ------------------------ Step 4: Date Range ------------------------\n",
    "start_date = datetime(2019, 7, 1)\n",
    "end_date = datetime(2025, 3, 30)\n",
    "all_dates_in_range = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Resume support: skip dates already processed\n",
    "if checkpoint_file.exists():\n",
    "    last_date_str = checkpoint_file.read_text().strip()\n",
    "    last_date = datetime.strptime(last_date_str, '%Y-%m-%d')\n",
    "    all_dates_in_range = [d for d in all_dates_in_range if d > last_date]\n",
    "\n",
    "# ------------------------ Step 5: Feature Extraction and CSV Writing ------------------------\n",
    "for day_d in tqdm(all_dates_in_range, desc=\"Processing dates\"):\n",
    "    utc_d = day_d - timedelta(hours=5, minutes=30)\n",
    "    feature_row_by_location = defaultdict(dict)\n",
    "\n",
    "    for init_time in init_times:\n",
    "        if init_time == '18':\n",
    "            init_day = (utc_d - timedelta(days=2)).strftime('%Y%m%d')\n",
    "        else:\n",
    "            init_day = (utc_d - timedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "        for var in variables:\n",
    "            for fhr in forecast_hours_needed[init_time]:\n",
    "                file = file_index[var].get((init_day, init_time), {}).get(fhr, None)\n",
    "                if not file or not os.path.exists(file):\n",
    "                    print(f\"Missing: {var} | {init_day} {init_time} | f{fhr:03d}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with Dataset(file, 'r') as ds:\n",
    "                        data = ds.variables[var][0, :, :]  # (lat, lon)\n",
    "                        for i, lat in enumerate(lats):\n",
    "                            for j, lon in enumerate(lons):\n",
    "                                key = (float(lat), float(lon))\n",
    "                                col_name = f\"{var}_{init_time}_f{fhr:03d}\"\n",
    "                                feature_row_by_location[key][col_name] = float(data[i, j])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Write to CSV immediately\n",
    "    for loc, row in feature_row_by_location.items():\n",
    "        row['date'] = day_d.strftime('%Y-%m-%d')\n",
    "        df_row = pd.DataFrame([row])\n",
    "        lat, lon = loc\n",
    "        out_path = os.path.join(output_dir, f\"lat_{lat}_lon_{lon}.csv\")\n",
    "        write_header = not os.path.exists(out_path)\n",
    "        df_row.to_csv(out_path, index=False, mode='a', header=write_header)\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_file.write_text(day_d.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904369e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
