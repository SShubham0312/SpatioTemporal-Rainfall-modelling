{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8d9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43b234",
   "metadata": {},
   "source": [
    "### finding stations with no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e780f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with NO missing dates from June to September (2019–2024):\n",
      "AMBERNATH_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "BHIWANDI_1918.0_7303.0.csv: lat=19.3, lon=73.05\n",
      "CANACONA_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "CHIPLUN_1732.0_7331.0.csv: lat=17.5333, lon=73.5167\n",
      "COLABA_-_IMD_OBSY_1854.0_7249.0.csv: lat=18.9, lon=72.8167\n",
      "DAHANU_-_IMD_OBSY_1958.0_7243.0.csv: lat=19.9667, lon=72.7167\n",
      "DAPOLI_AGRI_1746.0_7312.0.csv: lat=17.7667, lon=73.2\n",
      "DEVGAD_1623.0_7321.0.csv: lat=16.3833, lon=73.35\n",
      "DODAMARG_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "GAGANBAWADA_1633.0_7350.0.csv: lat=16.55, lon=73.8333\n",
      "GUHAGARH_1728.0_7312.0.csv: lat=17.4667, lon=73.2\n",
      "HARNAI_IMD_OBSY_1749.0_7306.0.csv: lat=17.8167, lon=73.1\n",
      "JAWHAR_1955.0_7314.0.csv: lat=19.9167, lon=73.2333\n",
      "KALYAN_1915.0_7307.0.csv: lat=19.25, lon=73.1167\n",
      "KANKAVLI_1616.0_7342.0.csv: lat=16.2667, lon=73.7\n",
      "KARJAT_AGRI_1855.0_7320.0.csv: lat=18.9167, lon=73.3333\n",
      "KHALAPUR_1852.0_7317.0.csv: lat=18.8667, lon=73.2833\n",
      "KUDAL_1601.0_7342.0.csv: lat=16.0167, lon=73.7\n",
      "LANJA_1652.0_7333.0.csv: lat=16.8667, lon=73.55\n",
      "MAHAD_1805.0_7325.0.csv: lat=18.0833, lon=73.4167\n",
      "MALVAN_1603.0_7328.0.csv: lat=16.05, lon=73.4667\n",
      "MANDANGAD_1759.0_7315.0.csv: lat=17.9833, lon=73.25\n",
      "MANGAON_1814.0_7317.0.csv: lat=18.2333, lon=73.2833\n",
      "MATHERAN_1859.0_7317.0.csv: lat=18.9833, lon=73.2833\n",
      "MHASLA_1808.0_7307.0.csv: lat=18.1333, lon=73.1167\n",
      "MOKHEDA_-_FMO_1956.0_7320.0.csv: lat=19.9333, lon=73.3333\n",
      "MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "MURBAD_1914.0_7324.0.csv: lat=19.2333, lon=73.4\n",
      "MURUD_1819.0_7258.0.csv: lat=18.3167, lon=72.9667\n",
      "PALGHAR_AGRI_1945.0_7241.0.csv: lat=19.75, lon=72.6833\n",
      "PANJIM_-_IMD_OBSY_1529.0_7349.0.csv: lat=15.4833, lon=73.8167\n",
      "PANVEL_AGRI_1859.0_7307.0.csv: lat=18.9833, lon=73.1167\n",
      "PEN_1844.0_7306.0.csv: lat=18.7333, lon=73.1\n",
      "PERNEM_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "POLADPUR_1759.0_7328.0.csv: lat=17.9833, lon=73.4667\n",
      "PURANDAR_SASVAD_1821.0_7402.0.csv: lat=18.35, lon=74.0333\n",
      "RAJAPUR_1639.0_7331.0.csv: lat=16.65, lon=73.5167\n",
      "RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv: lat=16.9833, lon=73.3333\n",
      "ROHA_1826.0_7307.0.csv: lat=18.4333, lon=73.1167\n",
      "SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv: lat=19.1167, lon=72.85\n",
      "SAWANTWADI_1554.0_7349.0.csv: lat=15.9, lon=73.8167\n",
      "SHAHAPUR_1927.0_7320.0.csv: lat=19.45, lon=73.3333\n",
      "SHRIWARDHAN_1803.0_7301.0.csv: lat=18.05, lon=73.0167\n",
      "SUDHAGAD_PALI_1832.0_7319.0.csv: lat=18.5333, lon=73.3167\n",
      "TALASARI_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "TALA_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "THANE_1912.0_7259.0.csv: lat=19.2, lon=72.9833\n",
      "ULHASNAGAR_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "URAN_1854.0_7255.0.csv: lat=18.9, lon=72.9167\n",
      "VAIBHAVWADI_9999.0_9999.0.csv: lat=100.65, lon=100.65\n",
      "VASAI_1920.0_7248.0.csv: lat=19.3333, lon=72.8\n",
      "VIKRAMGAD_1947.0_7104.0.csv: lat=19.7833, lon=71.0667\n",
      "WADA_1939.0_7308.0.csv: lat=19.65, lon=73.1333\n",
      "\n",
      "Total files with no missing dates: 54\n",
      "AMBERNATH_9999.0_9999.0.csv: 732 rows\n",
      "BHIWANDI_1918.0_7303.0.csv: 729 rows\n",
      "CANACONA_9999.0_9999.0.csv: 723 rows\n",
      "CHIPLUN_1732.0_7331.0.csv: 722 rows\n",
      "COLABA_-_IMD_OBSY_1854.0_7249.0.csv: 732 rows\n",
      "DAHANU_-_IMD_OBSY_1958.0_7243.0.csv: 731 rows\n",
      "DAPOLI_AGRI_1746.0_7312.0.csv: 718 rows\n",
      "DEVGAD_1623.0_7321.0.csv: 730 rows\n",
      "DODAMARG_9999.0_9999.0.csv: 732 rows\n",
      "GAGANBAWADA_1633.0_7350.0.csv: 726 rows\n",
      "GUHAGARH_1728.0_7312.0.csv: 721 rows\n",
      "HARNAI_IMD_OBSY_1749.0_7306.0.csv: 725 rows\n",
      "JAWHAR_1955.0_7314.0.csv: 719 rows\n",
      "KALYAN_1915.0_7307.0.csv: 730 rows\n",
      "KANKAVLI_1616.0_7342.0.csv: 731 rows\n",
      "KARJAT_AGRI_1855.0_7320.0.csv: 731 rows\n",
      "KHALAPUR_1852.0_7317.0.csv: 732 rows\n",
      "KUDAL_1601.0_7342.0.csv: 732 rows\n",
      "LANJA_1652.0_7333.0.csv: 728 rows\n",
      "MAHAD_1805.0_7325.0.csv: 722 rows\n",
      "MALVAN_1603.0_7328.0.csv: 732 rows\n",
      "MANDANGAD_1759.0_7315.0.csv: 729 rows\n",
      "MANGAON_1814.0_7317.0.csv: 732 rows\n",
      "MATHERAN_1859.0_7317.0.csv: 727 rows\n",
      "MHASLA_1808.0_7307.0.csv: 732 rows\n",
      "MOKHEDA_-_FMO_1956.0_7320.0.csv: 730 rows\n",
      "MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv: 732 rows\n",
      "MURBAD_1914.0_7324.0.csv: 729 rows\n",
      "MURUD_1819.0_7258.0.csv: 732 rows\n",
      "PALGHAR_AGRI_1945.0_7241.0.csv: 730 rows\n",
      "PANJIM_-_IMD_OBSY_1529.0_7349.0.csv: 731 rows\n",
      "PANVEL_AGRI_1859.0_7307.0.csv: 731 rows\n",
      "PEN_1844.0_7306.0.csv: 732 rows\n",
      "PERNEM_9999.0_9999.0.csv: 725 rows\n",
      "POLADPUR_1759.0_7328.0.csv: 732 rows\n",
      "PURANDAR_SASVAD_1821.0_7402.0.csv: 729 rows\n",
      "RAJAPUR_1639.0_7331.0.csv: 725 rows\n",
      "RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv: 732 rows\n",
      "ROHA_1826.0_7307.0.csv: 728 rows\n",
      "SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv: 726 rows\n",
      "SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv: 732 rows\n",
      "SAWANTWADI_1554.0_7349.0.csv: 731 rows\n",
      "SHAHAPUR_1927.0_7320.0.csv: 727 rows\n",
      "SHRIWARDHAN_1803.0_7301.0.csv: 732 rows\n",
      "SUDHAGAD_PALI_1832.0_7319.0.csv: 732 rows\n",
      "TALASARI_9999.0_9999.0.csv: 714 rows\n",
      "TALA_9999.0_9999.0.csv: 732 rows\n",
      "THANE_1912.0_7259.0.csv: 729 rows\n",
      "ULHASNAGAR_9999.0_9999.0.csv: 731 rows\n",
      "URAN_1854.0_7255.0.csv: 732 rows\n",
      "VAIBHAVWADI_9999.0_9999.0.csv: 731 rows\n",
      "VASAI_1920.0_7248.0.csv: 730 rows\n",
      "VIKRAMGAD_1947.0_7104.0.csv: 732 rows\n",
      "WADA_1939.0_7308.0.csv: 728 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_lat_lon(filename):\n",
    "    base = os.path.basename(filename).replace('.csv', '')\n",
    "    parts = base.split('_')\n",
    "    \n",
    "    try:\n",
    "        lat_str = parts[-2]\n",
    "        lon_str = parts[-1]\n",
    "\n",
    "        def convert(coord_str):\n",
    "            val = float(coord_str)\n",
    "            degrees = int(val // 100)\n",
    "            minutes = val - degrees * 100\n",
    "            return round(degrees + minutes / 60, 4)\n",
    "\n",
    "        return convert(lat_str), convert(lon_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing filename {filename}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def print_files_no_missing_dates_jun_to_sep(folder_path=\"select-stations\"):\n",
    "    full_date_range = pd.date_range(start='2019-05-01', end='2024-10-31')\n",
    "    full_date_range = full_date_range[full_date_range.month.isin([6,7,8,9])]\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    lat_lon_list = []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "\n",
    "        df_filtered = df[df['Date'].dt.month.isin([6,7,8,9])]\n",
    "        df_dates = df_filtered['Date']\n",
    "        missing_dates = full_date_range.difference(df_dates)\n",
    "\n",
    "        if len(missing_dates) <= 20:\n",
    "            lat, lon = extract_lat_lon(csv_file)\n",
    "            if lat is not None and lon is not None:\n",
    "                lat_lon_list.append((csv_file, lat, lon))\n",
    "\n",
    "    print(\"Files with NO missing dates from June to September (2019–2024):\")\n",
    "    for f, lat, lon in lat_lon_list:\n",
    "        print(f\"{f}: lat={lat}, lon={lon}\")\n",
    "    \n",
    "    print(f\"\\nTotal files with no missing dates: {len(lat_lon_list)}\")\n",
    "    return lat_lon_list\n",
    "\n",
    "\n",
    "# Run\n",
    "lat_lon_list = print_files_no_missing_dates_jun_to_sep(\"select-stations\")\n",
    "\n",
    "#print the length of each file \n",
    "for f, lat, lon in lat_lon_list:\n",
    "    file_path = os.path.join(\"select-stations\", f)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"{f}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44e473",
   "metadata": {},
   "source": [
    "- ### combining feature label for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "654f12ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R_H_L100_00_f018  R_H_L100_00_f021  R_H_L100_00_f024  R_H_L100_00_f027  \\\n",
      "0         97.901640         98.797032         97.718716         94.869518   \n",
      "1         98.705669         98.167657         98.437773         92.388017   \n",
      "2         98.489275         98.271643         97.167092         98.721086   \n",
      "3         97.646624         98.851686         96.931761         98.512505   \n",
      "4         99.344286         87.609273         98.603764         99.432101   \n",
      "\n",
      "   R_H_L100_00_f030  R_H_L100_00_f033  R_H_L100_00_f036  R_H_L100_00_f039  \\\n",
      "0         98.090332         96.404188         99.437046         98.947604   \n",
      "1         88.546134         91.951644         87.685549         83.421324   \n",
      "2         96.625117         99.226096         98.921860         98.473635   \n",
      "3         97.594515         88.123801         95.681743         90.930526   \n",
      "4         94.408516         88.805648         93.315889         95.219386   \n",
      "\n",
      "   R_H_L100_00_f042  TMP_L100_00_f018  ...  PRATE_L1_18_f030  \\\n",
      "0         96.992888        237.547548  ...          0.000274   \n",
      "1         91.438304        237.558135  ...          0.000000   \n",
      "2         98.250727        237.793688  ...          0.000197   \n",
      "3         85.798575        238.197360  ...          0.000206   \n",
      "4         95.712205        238.273260  ...          0.000000   \n",
      "\n",
      "   PRATE_L1_18_f033        PRATE_L1_18_f036  PRATE_L1_18_f039  \\\n",
      "0          0.000200                     0.0               0.0   \n",
      "1          0.000000                     0.0               0.0   \n",
      "2          0.000315  0.00019999999494757503               0.0   \n",
      "3          0.000128                     0.0               0.0   \n",
      "4          0.000000                     0.0               0.0   \n",
      "\n",
      "   PRATE_L1_18_f042  PRATE_L1_18_f045  PRATE_L1_18_f048        Date  Rainfall  \\\n",
      "0          0.000061          0.000000          0.000000  2019-07-01       0.0   \n",
      "1          0.000225          0.000000          0.000000  2019-07-02       0.0   \n",
      "2          0.000000          0.000672          0.000474  2019-07-03      45.0   \n",
      "3          0.000030          0.000000          0.000000  2019-07-04       0.0   \n",
      "4          0.000299          0.000323          0.000062  2019-07-05       0.0   \n",
      "\n",
      "                   Station  \n",
      "0  DHARMABAD_1853.0_7750.0  \n",
      "1  DHARMABAD_1853.0_7750.0  \n",
      "2  DHARMABAD_1853.0_7750.0  \n",
      "3  DHARMABAD_1853.0_7750.0  \n",
      "4  DHARMABAD_1853.0_7750.0  \n",
      "\n",
      "[5 rows x 183 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def standardize_date_col(df):\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'date':\n",
    "            df.rename(columns={col: 'Date'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def load_gfs_features(output_features_folder):\n",
    "    gfs_dfs = {}\n",
    "    for file in os.listdir(output_features_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(output_features_folder, file))\n",
    "            df = standardize_date_col(df)\n",
    "            # Extract lat, lon from filename: lat_<lat>_lon_<lon>.csv\n",
    "            parts = file.replace('.csv', '').split('_')\n",
    "            lat = float(parts[1])\n",
    "            lon = float(parts[3])\n",
    "            gfs_dfs[(lat, lon)] = df\n",
    "    return gfs_dfs\n",
    "\n",
    "def load_station_labels(per_station_folder, lat_lon_list):\n",
    "    station_dfs = {}\n",
    "    for filename, lat, lon in lat_lon_list:\n",
    "        path = os.path.join(per_station_folder, filename)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            df = standardize_date_col(df)\n",
    "            station_dfs[(lat, lon)] = df\n",
    "        else:\n",
    "            print(f\"Warning: Station file {path} does not exist.\")\n",
    "    return station_dfs\n",
    "\n",
    "def find_nearest_grid_points(station_coords, grid_coords, k=4):\n",
    "    tree = cKDTree(grid_coords)\n",
    "    dists, idxs = tree.query(station_coords, k=k)\n",
    "    return dists, idxs\n",
    "\n",
    "def interpolate_features_for_station(station_coord, grid_coords, gfs_dfs, idxs, dists):\n",
    "    weights = 1 / (dists + 1e-8)  # avoid div by zero\n",
    "    weights /= weights.sum()      # normalize weights to sum to 1\n",
    "\n",
    "    combined_features = None\n",
    "    for i, idx in enumerate(idxs):\n",
    "        grid_point = grid_coords[idx]\n",
    "        df = gfs_dfs[tuple(grid_point)].copy()\n",
    "        df = standardize_date_col(df)\n",
    "\n",
    "        # Multiply only numeric columns by weight\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "        if combined_features is None:\n",
    "            combined_features = df.copy()\n",
    "            combined_features[numeric_cols] = df[numeric_cols] * weights[i]\n",
    "        else:\n",
    "            combined_features[numeric_cols] += df[numeric_cols] * weights[i]\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "def create_combined_df(lat_lon_list, per_station_folder, output_features_folder):\n",
    "    gfs_dfs = load_gfs_features(output_features_folder)\n",
    "    station_dfs = load_station_labels(per_station_folder, lat_lon_list)\n",
    "\n",
    "    grid_coords = np.array(list(gfs_dfs.keys()))\n",
    "    station_coords = np.array([(lat, lon) for _, lat, lon in lat_lon_list])\n",
    "    dists, idxs = find_nearest_grid_points(station_coords, grid_coords, k=4)\n",
    "\n",
    "    combined_data = []\n",
    "\n",
    "    for i, ((filename, slat, slon), dist_list, idx_list) in enumerate(zip(lat_lon_list, dists, idxs)):\n",
    "\n",
    "        station_df = station_dfs.get((slat, slon))\n",
    "        if station_df is None:\n",
    "            print(f\"Skipping station {(slat, slon)} - data not found.\")\n",
    "            continue\n",
    "\n",
    "        # Interpolate features for this station\n",
    "        interp_features = interpolate_features_for_station(\n",
    "            (slat, slon), grid_coords, gfs_dfs, idx_list, dist_list\n",
    "        )\n",
    "\n",
    "        # Merge on Date column\n",
    "        merged_df = pd.merge(interp_features, station_df, on='Date', how='inner')\n",
    "\n",
    "        merged_df['Station'] = filename.replace('.csv', '')\n",
    "        combined_data.append(merged_df)\n",
    "\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# === Your given paths and lat_lon_list ===\n",
    "per_station_folder = 'per_station_data'\n",
    "output_features_folder = 'output_features'\n",
    "\n",
    "# Example lat_lon_list format: [('station1.csv', 12.34, 56.78), ('station2.csv', 12.35, 56.79), ...]\n",
    "# Make sure this list is properly defined in your code\n",
    "\n",
    "combined_df = create_combined_df(lat_lon_list, per_station_folder, output_features_folder)\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "707e1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put combined_df to a CSV file\n",
    "output_file = 'combined_interpolated_data.csv'\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a42556",
   "metadata": {},
   "source": [
    "- ### combining feature-label for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de9d9c",
   "metadata": {},
   "source": [
    "- kirging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f53ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station DINDORI_2012.0_7350.0.csv at (20.2, 73.8333)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DINDORI_2012.0_7350.0.csv: 100%|██████████| 731/731 [6:26:47<00:00, 31.75s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to station_outputs/subset_20.2_73.8333.csv\n",
      "Processing station MAHUR_9999.0_9999.0.csv at (100.65, 100.65)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAHUR_9999.0_9999.0.csv:   0%|          | 0/729 [00:00<?, ?it/s]/home/manoj/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/point.py:472: UserWarning: Latitude normalization has been prohibited in the newer versions of geopy, because the normalized value happened to be on a different pole, which is probably not what was meant. If you pass coordinates as positional args, please make sure that the order is (latitude, longitude) or (y, x) in Cartesian terms.\n",
      "  return cls(*args)\n",
      "MAHUR_9999.0_9999.0.csv:   0%|          | 0/729 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Latitude must be in the [-90; 90] range.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# lat_lon_list = [('station1.csv', 19.07, 72.88), ('station2.csv', 18.96, 72.83)]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43msave_station_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat_lon_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mper_station_data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput_features\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36msave_station_outputs\u001b[39m\u001b[34m(lat_lon_list, per_station_folder, output_features_folder, output_dir)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(station_df.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(station_df), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m    106\u001b[39m     date_val = row[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     interp_feats = \u001b[43minterpolate_features_for_station\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfs_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(pd.isna(\u001b[38;5;28mlist\u001b[39m(interp_feats.values()))):\n\u001b[32m    109\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36minterpolate_features_for_station\u001b[39m\u001b[34m(station_coord, date_str, gfs_dfs, grid_coords, feature_names)\u001b[39m\n\u001b[32m     80\u001b[39m interpolated = {}\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_names:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     kriging_model = \u001b[43mapply_kriging_to_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgfs_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation_coord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kriging_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m         interpolated[feature] = np.nan\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mapply_kriging_to_features\u001b[39m\u001b[34m(date_str, feature_name, grid_coords, gfs_dfs, station_coord, k)\u001b[39m\n\u001b[32m     41\u001b[39m distances = []\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (glat, glon) \u001b[38;5;129;01min\u001b[39;00m grid_coords:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     dist = \u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_coord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mglat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.kilometers\n\u001b[32m     44\u001b[39m     distances.append(((glat, glon), dist))\n\u001b[32m     46\u001b[39m nearest_points = \u001b[38;5;28msorted\u001b[39m(distances, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m])[:k]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/distance.py:540\u001b[39m, in \u001b[36mgeodesic.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28mself\u001b[39m.set_ellipsoid(kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mellipsoid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mWGS-84\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m    539\u001b[39m major, minor, f = \u001b[38;5;28mself\u001b[39m.ELLIPSOID\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/distance.py:276\u001b[39m, in \u001b[36mDistance.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m1\u001b[39m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m util.pairwise(args):\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m         kilometers += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m kilometers += units.kilometers(**kwargs)\n\u001b[32m    279\u001b[39m \u001b[38;5;28mself\u001b[39m.__kilometers = kilometers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/distance.py:556\u001b[39m, in \u001b[36mgeodesic.measure\u001b[39m\u001b[34m(self, a, b)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeasure\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     a, b = \u001b[43mPoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m, Point(b)\n\u001b[32m    557\u001b[39m     _ensure_same_altitude(a, b)\n\u001b[32m    558\u001b[39m     lat1, lon1 = a.latitude, a.longitude\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/point.py:175\u001b[39m, in \u001b[36mPoint.__new__\u001b[39m\u001b[34m(cls, latitude, longitude, altitude)\u001b[39m\n\u001b[32m    171\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    172\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to create Point instance from \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % (arg,)\n\u001b[32m    173\u001b[39m             )\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_arg:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    179\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA single number has been passed to the Point \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconstructor. This is probably a mistake, because \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mto get rid of this error.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    185\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/point.py:472\u001b[39m, in \u001b[36mPoint.from_sequence\u001b[39m\u001b[34m(cls, seq)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m3\u001b[39m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mWhen creating a Point from sequence, it \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    471\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33mmust not have more than 3 items.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/point.py:188\u001b[39m, in \u001b[36mPoint.__new__\u001b[39m\u001b[34m(cls, latitude, longitude, altitude)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_arg:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    179\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA single number has been passed to the Point \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconstructor. This is probably a mistake, because \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mto get rid of this error.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    187\u001b[39m latitude, longitude, altitude = \\\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43m_normalize_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maltitude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28mself\u001b[39m.latitude = latitude\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MH/.venv/lib/python3.12/site-packages/geopy/point.py:74\u001b[39m, in \u001b[36m_normalize_coordinates\u001b[39m\u001b[34m(latitude, longitude, altitude)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(latitude) > \u001b[32m90\u001b[39m:\n\u001b[32m     67\u001b[39m     warnings.warn(\u001b[33m'\u001b[39m\u001b[33mLatitude normalization has been prohibited in the newer \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     68\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mversions of geopy, because the normalized value happened \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     69\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33mto be on a different pole, which is probably not what was \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m                   \u001b[33m'\u001b[39m\u001b[33m(latitude, longitude) or (y, x) in Cartesian terms.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     73\u001b[39m                   \u001b[38;5;167;01mUserWarning\u001b[39;00m, stacklevel=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mLatitude must be in the [-90; 90] range.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(longitude) > \u001b[32m180\u001b[39m:\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# Longitude normalization is pretty straightforward and doesn't seem\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# to be error-prone, so there's nothing to complain about.\u001b[39;00m\n\u001b[32m     79\u001b[39m     longitude = _normalize_angle(longitude, \u001b[32m180.0\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Latitude must be in the [-90; 90] range."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def load_gfs_features(output_features_folder):\n",
    "    gfs_dfs = {}\n",
    "    for file in os.listdir(output_features_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(output_features_folder, file))\n",
    "            parts = file.replace('.csv', '').split('_')\n",
    "            lat = float(parts[1])\n",
    "            lon = float(parts[3])\n",
    "            gfs_dfs[(lat, lon)] = df\n",
    "    return gfs_dfs\n",
    "\n",
    "def load_station_labels(per_station_folder, lat_lon_list):\n",
    "    station_dfs = {}\n",
    "    for filename, lat, lon in lat_lon_list:\n",
    "        path = os.path.join(per_station_folder, filename)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            if 'Date' not in df.columns and 'date' in df.columns:\n",
    "                df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "            station_dfs[(lat, lon)] = df\n",
    "    return station_dfs\n",
    "\n",
    "def standardize_date_col(df):\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    elif 'date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['date'])\n",
    "    else:\n",
    "        raise ValueError(\"No Date column found\")\n",
    "    return df\n",
    "\n",
    "def apply_kriging_to_features(date_str, feature_name, grid_coords, gfs_dfs, station_coord, k=30):\n",
    "    distances = []\n",
    "    for (glat, glon) in grid_coords:\n",
    "        dist = geodesic(station_coord, (glat, glon)).kilometers\n",
    "        distances.append(((glat, glon), dist))\n",
    "\n",
    "    nearest_points = sorted(distances, key=lambda x: x[1])[:k]\n",
    "\n",
    "    values = []\n",
    "    lats = []\n",
    "    lons = []\n",
    "\n",
    "    for (glat, glon), _ in nearest_points:\n",
    "        df = gfs_dfs[(glat, glon)]\n",
    "        df = standardize_date_col(df)\n",
    "        row = df.loc[df['Date'] == date_str]\n",
    "        if not row.empty:\n",
    "            val = row.iloc[0][feature_name]\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            values.append(val)\n",
    "            lats.append(glat)\n",
    "            lons.append(glon)\n",
    "\n",
    "    if len(values) < 3:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        OK = OrdinaryKriging(\n",
    "            lons, lats, values,\n",
    "            variogram_model='linear',\n",
    "            verbose=False,\n",
    "            enable_plotting=False\n",
    "        )\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return OK\n",
    "\n",
    "def interpolate_features_for_station(station_coord, date_str, gfs_dfs, grid_coords, feature_names):\n",
    "    interpolated = {}\n",
    "    for feature in feature_names:\n",
    "        kriging_model = apply_kriging_to_features(date_str, feature, grid_coords, gfs_dfs, station_coord)\n",
    "        if kriging_model is None:\n",
    "            interpolated[feature] = np.nan\n",
    "        else:\n",
    "            z, ss = kriging_model.execute('points', [station_coord[1]], [station_coord[0]])\n",
    "            interpolated[feature] = z[0]\n",
    "    return interpolated\n",
    "\n",
    "def save_station_outputs(lat_lon_list, per_station_folder, output_features_folder, output_dir='station_outputs'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    gfs_dfs = load_gfs_features(output_features_folder)\n",
    "    station_dfs = load_station_labels(per_station_folder, lat_lon_list)\n",
    "    grid_coords = np.array(list(gfs_dfs.keys()))\n",
    "    feature_names = list(next(iter(gfs_dfs.values())).columns)\n",
    "    feature_names = [f for f in feature_names if f != 'Date']\n",
    "\n",
    "    for filename, slat, slon in lat_lon_list:\n",
    "        print(f\"Processing station {filename} at ({slat}, {slon})...\")\n",
    "        station_df = station_dfs[(slat, slon)]\n",
    "        station_df = standardize_date_col(station_df)\n",
    "\n",
    "        rows = []\n",
    "        for idx, row in tqdm(station_df.iterrows(), total=len(station_df), desc=f\"{filename}\"):\n",
    "            date_val = row['Date']\n",
    "            interp_feats = interpolate_features_for_station((slat, slon), date_val, gfs_dfs, grid_coords, feature_names)\n",
    "            if any(pd.isna(list(interp_feats.values()))):\n",
    "                continue\n",
    "            out_row = interp_feats.copy()\n",
    "            out_row['Date'] = date_val\n",
    "            out_row['Rainfall'] = row['Rainfall'] if 'Rainfall' in row else np.nan\n",
    "            out_row['station_lat'] = slat\n",
    "            out_row['station_lon'] = slon\n",
    "            out_row['month'] = date_val.month\n",
    "            out_row['dayofyear'] = date_val.dayofyear\n",
    "            rows.append(out_row)\n",
    "\n",
    "        out_df = pd.DataFrame(rows)\n",
    "        out_path = os.path.join(output_dir, f\"subset_{slat}_{slon}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"Saved to {out_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# lat_lon_list = [('station1.csv', 19.07, 72.88), ('station2.csv', 18.96, 72.83)]\n",
    "save_station_outputs(lat_lon_list, 'per_station_data', 'output_features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54389452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stations processed:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station DHARMABAD_1853.0_7750.0.csv at (18.8833, 77.8333)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x79ba51b29250>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manoj/Desktop/MH/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from datetime import datetime\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def load_gfs_features(output_features_folder):\n",
    "    gfs_dfs = {}\n",
    "    for file in os.listdir(output_features_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(output_features_folder, file))\n",
    "            parts = file.replace('.csv', '').split('_')\n",
    "            lat = float(parts[1])\n",
    "            lon = float(parts[3])\n",
    "            gfs_dfs[(lat, lon)] = df\n",
    "    return gfs_dfs\n",
    "\n",
    "def load_station_labels(per_station_folder, lat_lon_list):\n",
    "    station_dfs = {}\n",
    "    for filename, lat, lon in lat_lon_list:\n",
    "        path = os.path.join(per_station_folder, filename)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            if 'Date' not in df.columns and 'date' in df.columns:\n",
    "                df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "            station_dfs[(lat, lon)] = df\n",
    "    return station_dfs\n",
    "\n",
    "def standardize_date_col(df):\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "    elif 'date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['date'])\n",
    "    else:\n",
    "        raise ValueError(\"No Date column found\")\n",
    "    return df\n",
    "\n",
    "def apply_local_kriging(date_str, feature_name, station_coord, gfs_dfs, grid_coords, k=30):\n",
    "    # Collect all grid points values for the date and feature\n",
    "    values = []\n",
    "    lats = []\n",
    "    lons = []\n",
    "    for (glat, glon), df in gfs_dfs.items():\n",
    "        df = standardize_date_col(df)\n",
    "        row = df.loc[df['Date'] == date_str]\n",
    "        if not row.empty:\n",
    "            val = row.iloc[0][feature_name]\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            values.append(val)\n",
    "            lats.append(glat)\n",
    "            lons.append(glon)\n",
    "\n",
    "    if len(values) < 3:\n",
    "        # Not enough points to krige\n",
    "        return np.nan\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    lats = np.array(lats)\n",
    "    lons = np.array(lons)\n",
    "    values = np.array(values)\n",
    "\n",
    "    # Find nearest k neighbors to the station coordinate\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k, len(lats)), algorithm='auto')\n",
    "    coords = np.column_stack((lats, lons))\n",
    "    nbrs.fit(coords)\n",
    "    dist, idx = nbrs.kneighbors(np.array([station_coord]))\n",
    "\n",
    "    # Select local neighborhood points\n",
    "    local_lats = lats[idx[0]]\n",
    "    local_lons = lons[idx[0]]\n",
    "    local_vals = values[idx[0]]\n",
    "\n",
    "    # Fit kriging on local neighbors\n",
    "    try:\n",
    "        OK = OrdinaryKriging(\n",
    "            local_lons,\n",
    "            local_lats,\n",
    "            local_vals,\n",
    "            variogram_model='linear',\n",
    "            verbose=False,\n",
    "            enable_plotting=False,\n",
    "        )\n",
    "        z, ss = OK.execute('points', [station_coord[1]], [station_coord[0]])\n",
    "        return z[0]\n",
    "    except Exception as e:\n",
    "        # If kriging fails fallback to nan\n",
    "        return np.nan\n",
    "\n",
    "def interpolate_features_for_station(station_coord, date_str, gfs_dfs, grid_coords, feature_names, k=30):\n",
    "    interpolated = {}\n",
    "    for feature in feature_names:\n",
    "        val = apply_local_kriging(date_str, feature, station_coord, gfs_dfs, grid_coords, k=k)\n",
    "        interpolated[feature] = val\n",
    "    return interpolated\n",
    "\n",
    "from tqdm import tqdm  # Add this import at the top\n",
    "\n",
    "def save_station_outputs(lat_lon_list, per_station_folder, output_features_folder, output_dir='station_outputs', k=30):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    gfs_dfs = load_gfs_features(output_features_folder)\n",
    "    station_dfs = load_station_labels(per_station_folder, lat_lon_list)\n",
    "    grid_coords = np.array(list(gfs_dfs.keys()))\n",
    "    feature_names = list(next(iter(gfs_dfs.values())).columns)\n",
    "    feature_names = [f for f in feature_names if f not in ['Date']]  # exclude Date\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    for filename, slat, slon in tqdm(lat_lon_list, desc=\"Stations processed\"):\n",
    "        print(f\"Processing station {filename} at ({slat}, {slon})...\")\n",
    "        station_df = station_dfs[(slat, slon)]\n",
    "        station_df = standardize_date_col(station_df)\n",
    "        \n",
    "        rows = []\n",
    "        for idx, row in station_df.iterrows():\n",
    "            date_val = row['Date']\n",
    "            interp_feats = interpolate_features_for_station((slat, slon), date_val, gfs_dfs, grid_coords, feature_names, k=k)\n",
    "            if any(pd.isna(list(interp_feats.values()))):\n",
    "                continue\n",
    "            \n",
    "            out_row = interp_feats.copy()\n",
    "            out_row['Date'] = date_val\n",
    "            out_row['Rainfall'] = row['Rainfall'] if 'Rainfall' in row else np.nan\n",
    "            out_row['station_lat'] = slat\n",
    "            out_row['station_lon'] = slon\n",
    "            out_row['month'] = date_val.month\n",
    "            out_row['dayofyear'] = date_val.dayofyear\n",
    "            \n",
    "            rows.append(out_row)\n",
    "        \n",
    "        out_df = pd.DataFrame(rows)\n",
    "        out_path = os.path.join(output_dir, f\"subset_{slat}_{slon}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        print(f\"Saved to {out_path}\")\n",
    "\n",
    "# Usage example:\n",
    "# lat_lon_list = [('station1.csv', 19.07, 72.88), ('station2.csv', 18.96, 72.83)]\n",
    "save_station_outputs(lat_lon_list, per_station_folder, output_features_folder, k=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8cac8",
   "metadata": {},
   "source": [
    "- IDW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e997fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def standardize_date_col(df):\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'date':\n",
    "            df.rename(columns={col: 'Date'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def load_gfs_features(output_features_folder):\n",
    "    gfs_dfs = {}\n",
    "    for file in os.listdir(output_features_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(output_features_folder, file))\n",
    "            df = standardize_date_col(df)\n",
    "            parts = file.replace('.csv', '').split('_')\n",
    "            lat = float(parts[1])\n",
    "            lon = float(parts[3])\n",
    "            gfs_dfs[(lat, lon)] = df\n",
    "    return gfs_dfs\n",
    "\n",
    "def load_station_labels(per_station_folder, lat_lon_list):\n",
    "    station_dfs = {}\n",
    "    for filename, lat, lon in lat_lon_list:\n",
    "        path = os.path.join(per_station_folder, filename)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            df = standardize_date_col(df)\n",
    "            station_dfs[(lat, lon)] = df\n",
    "        else:\n",
    "            print(f\"Warning: Station file {path} does not exist.\")\n",
    "    return station_dfs\n",
    "\n",
    "\n",
    "def find_nearest_grid_points(station_coords, grid_coords, k=(len(lat_lon_list) - 1)):\n",
    "    tree = cKDTree(grid_coords)\n",
    "    dists, idxs = tree.query(station_coords, k=k)\n",
    "    return dists, idxs\n",
    "\n",
    "def interpolate_features_for_station(station_coord, grid_coords, gfs_dfs, idxs, dists):\n",
    "    weights = 1 / (dists + 1e-8)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    combined_features = None\n",
    "    for i, idx in enumerate(idxs):\n",
    "        grid_point = grid_coords[idx]\n",
    "        df = gfs_dfs[tuple(grid_point)].copy()\n",
    "        df = standardize_date_col(df)\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "        if combined_features is None:\n",
    "            combined_features = df.copy()\n",
    "            combined_features[numeric_cols] = df[numeric_cols] * weights[i]\n",
    "        else:\n",
    "            combined_features[numeric_cols] += df[numeric_cols] * weights[i]\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "def generate_station_wise_outputs(lat_lon_list, per_station_folder, output_features_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    gfs_dfs = load_gfs_features(output_features_folder)\n",
    "    station_dfs = load_station_labels(per_station_folder, lat_lon_list)\n",
    "\n",
    "    grid_coords = np.array(list(gfs_dfs.keys()))\n",
    "    station_coords = np.array([(lat, lon) for _, lat, lon in lat_lon_list])\n",
    "    dists, idxs = find_nearest_grid_points(station_coords, grid_coords, k=4)\n",
    "\n",
    "    for (filename, slat, slon), dist_list, idx_list in zip(lat_lon_list, dists, idxs):\n",
    "        station_df = station_dfs.get((slat, slon))\n",
    "        if station_df is None:\n",
    "            print(f\"Skipping station {(slat, slon)} - data not found.\")\n",
    "            continue\n",
    "\n",
    "        interp_features = interpolate_features_for_station(\n",
    "            (slat, slon), grid_coords, gfs_dfs, idx_list, dist_list\n",
    "        )\n",
    "\n",
    "        merged_df = pd.merge(interp_features, station_df, on='Date', how='inner')\n",
    "        merged_df['Station'] = filename.replace('.csv', '')\n",
    "\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d8bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: feature-label-comb\\AMBERNATH_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\BHIWANDI_1918.0_7303.0.csv\n",
      "Saved: feature-label-comb\\CANACONA_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\CHIPLUN_1732.0_7331.0.csv\n",
      "Saved: feature-label-comb\\CANACONA_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\CHIPLUN_1732.0_7331.0.csv\n",
      "Saved: feature-label-comb\\COLABA_-_IMD_OBSY_1854.0_7249.0.csv\n",
      "Saved: feature-label-comb\\DAHANU_-_IMD_OBSY_1958.0_7243.0.csv\n",
      "Saved: feature-label-comb\\COLABA_-_IMD_OBSY_1854.0_7249.0.csv\n",
      "Saved: feature-label-comb\\DAHANU_-_IMD_OBSY_1958.0_7243.0.csv\n",
      "Saved: feature-label-comb\\DAPOLI_AGRI_1746.0_7312.0.csv\n",
      "Saved: feature-label-comb\\DAPOLI_AGRI_1746.0_7312.0.csv\n",
      "Saved: feature-label-comb\\DEVGAD_1623.0_7321.0.csv\n",
      "Saved: feature-label-comb\\DODAMARG_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\DEVGAD_1623.0_7321.0.csv\n",
      "Saved: feature-label-comb\\DODAMARG_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\GAGANBAWADA_1633.0_7350.0.csv\n",
      "Saved: feature-label-comb\\GUHAGARH_1728.0_7312.0.csv\n",
      "Saved: feature-label-comb\\GAGANBAWADA_1633.0_7350.0.csv\n",
      "Saved: feature-label-comb\\GUHAGARH_1728.0_7312.0.csv\n",
      "Saved: feature-label-comb\\HARNAI_IMD_OBSY_1749.0_7306.0.csv\n",
      "Saved: feature-label-comb\\HARNAI_IMD_OBSY_1749.0_7306.0.csv\n",
      "Saved: feature-label-comb\\JAWHAR_1955.0_7314.0.csv\n",
      "Saved: feature-label-comb\\KALYAN_1915.0_7307.0.csv\n",
      "Saved: feature-label-comb\\JAWHAR_1955.0_7314.0.csv\n",
      "Saved: feature-label-comb\\KALYAN_1915.0_7307.0.csv\n",
      "Saved: feature-label-comb\\KANKAVLI_1616.0_7342.0.csv\n",
      "Saved: feature-label-comb\\KARJAT_AGRI_1855.0_7320.0.csv\n",
      "Saved: feature-label-comb\\KANKAVLI_1616.0_7342.0.csv\n",
      "Saved: feature-label-comb\\KARJAT_AGRI_1855.0_7320.0.csv\n",
      "Saved: feature-label-comb\\KHALAPUR_1852.0_7317.0.csv\n",
      "Saved: feature-label-comb\\KUDAL_1601.0_7342.0.csv\n",
      "Saved: feature-label-comb\\KHALAPUR_1852.0_7317.0.csv\n",
      "Saved: feature-label-comb\\KUDAL_1601.0_7342.0.csv\n",
      "Saved: feature-label-comb\\LANJA_1652.0_7333.0.csv\n",
      "Saved: feature-label-comb\\MAHAD_1805.0_7325.0.csv\n",
      "Saved: feature-label-comb\\LANJA_1652.0_7333.0.csv\n",
      "Saved: feature-label-comb\\MAHAD_1805.0_7325.0.csv\n",
      "Saved: feature-label-comb\\MALVAN_1603.0_7328.0.csv\n",
      "Saved: feature-label-comb\\MANDANGAD_1759.0_7315.0.csv\n",
      "Saved: feature-label-comb\\MALVAN_1603.0_7328.0.csv\n",
      "Saved: feature-label-comb\\MANDANGAD_1759.0_7315.0.csv\n",
      "Saved: feature-label-comb\\MANGAON_1814.0_7317.0.csv\n",
      "Saved: feature-label-comb\\MATHERAN_1859.0_7317.0.csv\n",
      "Saved: feature-label-comb\\MANGAON_1814.0_7317.0.csv\n",
      "Saved: feature-label-comb\\MATHERAN_1859.0_7317.0.csv\n",
      "Saved: feature-label-comb\\MHASLA_1808.0_7307.0.csv\n",
      "Saved: feature-label-comb\\MOKHEDA_-_FMO_1956.0_7320.0.csv\n",
      "Saved: feature-label-comb\\MHASLA_1808.0_7307.0.csv\n",
      "Saved: feature-label-comb\\MOKHEDA_-_FMO_1956.0_7320.0.csv\n",
      "Saved: feature-label-comb\\MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\MURBAD_1914.0_7324.0.csv\n",
      "Saved: feature-label-comb\\MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\MURBAD_1914.0_7324.0.csv\n",
      "Saved: feature-label-comb\\MURUD_1819.0_7258.0.csv\n",
      "Saved: feature-label-comb\\PALGHAR_AGRI_1945.0_7241.0.csv\n",
      "Saved: feature-label-comb\\MURUD_1819.0_7258.0.csv\n",
      "Saved: feature-label-comb\\PALGHAR_AGRI_1945.0_7241.0.csv\n",
      "Saved: feature-label-comb\\PANJIM_-_IMD_OBSY_1529.0_7349.0.csv\n",
      "Saved: feature-label-comb\\PANVEL_AGRI_1859.0_7307.0.csv\n",
      "Saved: feature-label-comb\\PANJIM_-_IMD_OBSY_1529.0_7349.0.csv\n",
      "Saved: feature-label-comb\\PANVEL_AGRI_1859.0_7307.0.csv\n",
      "Saved: feature-label-comb\\PEN_1844.0_7306.0.csv\n",
      "Saved: feature-label-comb\\PERNEM_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\PEN_1844.0_7306.0.csv\n",
      "Saved: feature-label-comb\\PERNEM_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\POLADPUR_1759.0_7328.0.csv\n",
      "Saved: feature-label-comb\\PURANDAR_SASVAD_1821.0_7402.0.csv\n",
      "Saved: feature-label-comb\\POLADPUR_1759.0_7328.0.csv\n",
      "Saved: feature-label-comb\\PURANDAR_SASVAD_1821.0_7402.0.csv\n",
      "Saved: feature-label-comb\\RAJAPUR_1639.0_7331.0.csv\n",
      "Saved: feature-label-comb\\RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv\n",
      "Saved: feature-label-comb\\RAJAPUR_1639.0_7331.0.csv\n",
      "Saved: feature-label-comb\\RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv\n",
      "Saved: feature-label-comb\\ROHA_1826.0_7307.0.csv\n",
      "Saved: feature-label-comb\\SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\ROHA_1826.0_7307.0.csv\n",
      "Saved: feature-label-comb\\SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv\n",
      "Saved: feature-label-comb\\SAWANTWADI_1554.0_7349.0.csv\n",
      "Saved: feature-label-comb\\SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv\n",
      "Saved: feature-label-comb\\SAWANTWADI_1554.0_7349.0.csv\n",
      "Saved: feature-label-comb\\SHAHAPUR_1927.0_7320.0.csv\n",
      "Saved: feature-label-comb\\SHAHAPUR_1927.0_7320.0.csv\n",
      "Saved: feature-label-comb\\SHRIWARDHAN_1803.0_7301.0.csv\n",
      "Saved: feature-label-comb\\SUDHAGAD_PALI_1832.0_7319.0.csv\n",
      "Saved: feature-label-comb\\SHRIWARDHAN_1803.0_7301.0.csv\n",
      "Saved: feature-label-comb\\SUDHAGAD_PALI_1832.0_7319.0.csv\n",
      "Saved: feature-label-comb\\TALASARI_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\TALA_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\TALASARI_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\TALA_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\THANE_1912.0_7259.0.csv\n",
      "Saved: feature-label-comb\\ULHASNAGAR_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\THANE_1912.0_7259.0.csv\n",
      "Saved: feature-label-comb\\ULHASNAGAR_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\URAN_1854.0_7255.0.csv\n",
      "Saved: feature-label-comb\\VAIBHAVWADI_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\URAN_1854.0_7255.0.csv\n",
      "Saved: feature-label-comb\\VAIBHAVWADI_9999.0_9999.0.csv\n",
      "Saved: feature-label-comb\\VASAI_1920.0_7248.0.csv\n",
      "Saved: feature-label-comb\\VIKRAMGAD_1947.0_7104.0.csv\n",
      "Saved: feature-label-comb\\VASAI_1920.0_7248.0.csv\n",
      "Saved: feature-label-comb\\VIKRAMGAD_1947.0_7104.0.csv\n",
      "Saved: feature-label-comb\\WADA_1939.0_7308.0.csv\n",
      "AMBERNATH_9999.0_9999.0.csv: 700 rows\n",
      "BHIWANDI_1918.0_7303.0.csv: 698 rows\n",
      "CANACONA_9999.0_9999.0.csv: 700 rows\n",
      "CHIPLUN_1732.0_7331.0.csv: 692 rows\n",
      "COLABA_-_IMD_OBSY_1854.0_7249.0.csv: 701 rows\n",
      "DAHANU_-_IMD_OBSY_1958.0_7243.0.csv: 700 rows\n",
      "DAPOLI_AGRI_1746.0_7312.0.csv: 693 rows\n",
      "DEVGAD_1623.0_7321.0.csv: 699 rows\n",
      "DODAMARG_9999.0_9999.0.csv: 700 rows\n",
      "GAGANBAWADA_1633.0_7350.0.csv: 695 rows\n",
      "GUHAGARH_1728.0_7312.0.csv: 691 rows\n",
      "Saved: feature-label-comb\\WADA_1939.0_7308.0.csv\n",
      "AMBERNATH_9999.0_9999.0.csv: 700 rows\n",
      "BHIWANDI_1918.0_7303.0.csv: 698 rows\n",
      "CANACONA_9999.0_9999.0.csv: 700 rows\n",
      "CHIPLUN_1732.0_7331.0.csv: 692 rows\n",
      "COLABA_-_IMD_OBSY_1854.0_7249.0.csv: 701 rows\n",
      "DAHANU_-_IMD_OBSY_1958.0_7243.0.csv: 700 rows\n",
      "DAPOLI_AGRI_1746.0_7312.0.csv: 693 rows\n",
      "DEVGAD_1623.0_7321.0.csv: 699 rows\n",
      "DODAMARG_9999.0_9999.0.csv: 700 rows\n",
      "GAGANBAWADA_1633.0_7350.0.csv: 695 rows\n",
      "GUHAGARH_1728.0_7312.0.csv: 691 rows\n",
      "HARNAI_IMD_OBSY_1749.0_7306.0.csv: 695 rows\n",
      "JAWHAR_1955.0_7314.0.csv: 688 rows\n",
      "KALYAN_1915.0_7307.0.csv: 699 rows\n",
      "KANKAVLI_1616.0_7342.0.csv: 700 rows\n",
      "KARJAT_AGRI_1855.0_7320.0.csv: 701 rows\n",
      "KHALAPUR_1852.0_7317.0.csv: 701 rows\n",
      "KUDAL_1601.0_7342.0.csv: 701 rows\n",
      "LANJA_1652.0_7333.0.csv: 698 rows\n",
      "MAHAD_1805.0_7325.0.csv: 691 rows\n",
      "MALVAN_1603.0_7328.0.csv: 701 rows\n",
      "MANDANGAD_1759.0_7315.0.csv: 700 rows\n",
      "MANGAON_1814.0_7317.0.csv: 701 rows\n",
      "HARNAI_IMD_OBSY_1749.0_7306.0.csv: 695 rows\n",
      "JAWHAR_1955.0_7314.0.csv: 688 rows\n",
      "KALYAN_1915.0_7307.0.csv: 699 rows\n",
      "KANKAVLI_1616.0_7342.0.csv: 700 rows\n",
      "KARJAT_AGRI_1855.0_7320.0.csv: 701 rows\n",
      "KHALAPUR_1852.0_7317.0.csv: 701 rows\n",
      "KUDAL_1601.0_7342.0.csv: 701 rows\n",
      "LANJA_1652.0_7333.0.csv: 698 rows\n",
      "MAHAD_1805.0_7325.0.csv: 691 rows\n",
      "MALVAN_1603.0_7328.0.csv: 701 rows\n",
      "MANDANGAD_1759.0_7315.0.csv: 700 rows\n",
      "MANGAON_1814.0_7317.0.csv: 701 rows\n",
      "MATHERAN_1859.0_7317.0.csv: 696 rows\n",
      "MHASLA_1808.0_7307.0.csv: 701 rows\n",
      "MOKHEDA_-_FMO_1956.0_7320.0.csv: 700 rows\n",
      "MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv: 700 rows\n",
      "MURBAD_1914.0_7324.0.csv: 698 rows\n",
      "MURUD_1819.0_7258.0.csv: 701 rows\n",
      "PALGHAR_AGRI_1945.0_7241.0.csv: 699 rows\n",
      "PANJIM_-_IMD_OBSY_1529.0_7349.0.csv: 700 rows\n",
      "PANVEL_AGRI_1859.0_7307.0.csv: 701 rows\n",
      "PEN_1844.0_7306.0.csv: 701 rows\n",
      "PERNEM_9999.0_9999.0.csv: 700 rows\n",
      "POLADPUR_1759.0_7328.0.csv: 701 rows\n",
      "MATHERAN_1859.0_7317.0.csv: 696 rows\n",
      "MHASLA_1808.0_7307.0.csv: 701 rows\n",
      "MOKHEDA_-_FMO_1956.0_7320.0.csv: 700 rows\n",
      "MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv: 700 rows\n",
      "MURBAD_1914.0_7324.0.csv: 698 rows\n",
      "MURUD_1819.0_7258.0.csv: 701 rows\n",
      "PALGHAR_AGRI_1945.0_7241.0.csv: 699 rows\n",
      "PANJIM_-_IMD_OBSY_1529.0_7349.0.csv: 700 rows\n",
      "PANVEL_AGRI_1859.0_7307.0.csv: 701 rows\n",
      "PEN_1844.0_7306.0.csv: 701 rows\n",
      "PERNEM_9999.0_9999.0.csv: 700 rows\n",
      "POLADPUR_1759.0_7328.0.csv: 701 rows\n",
      "PURANDAR_SASVAD_1821.0_7402.0.csv: 701 rows\n",
      "RAJAPUR_1639.0_7331.0.csv: 695 rows\n",
      "RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv: 701 rows\n",
      "ROHA_1826.0_7307.0.csv: 697 rows\n",
      "SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv: 700 rows\n",
      "SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv: 701 rows\n",
      "SAWANTWADI_1554.0_7349.0.csv: 700 rows\n",
      "SHAHAPUR_1927.0_7320.0.csv: 696 rows\n",
      "SHRIWARDHAN_1803.0_7301.0.csv: 701 rows\n",
      "SUDHAGAD_PALI_1832.0_7319.0.csv: 701 rows\n",
      "TALASARI_9999.0_9999.0.csv: 700 rows\n",
      "TALA_9999.0_9999.0.csv: 700 rows\n",
      "THANE_1912.0_7259.0.csv: 698 rows\n",
      "PURANDAR_SASVAD_1821.0_7402.0.csv: 701 rows\n",
      "RAJAPUR_1639.0_7331.0.csv: 695 rows\n",
      "RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv: 701 rows\n",
      "ROHA_1826.0_7307.0.csv: 697 rows\n",
      "SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv: 700 rows\n",
      "SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv: 701 rows\n",
      "SAWANTWADI_1554.0_7349.0.csv: 700 rows\n",
      "SHAHAPUR_1927.0_7320.0.csv: 696 rows\n",
      "SHRIWARDHAN_1803.0_7301.0.csv: 701 rows\n",
      "SUDHAGAD_PALI_1832.0_7319.0.csv: 701 rows\n",
      "TALASARI_9999.0_9999.0.csv: 700 rows\n",
      "TALA_9999.0_9999.0.csv: 700 rows\n",
      "THANE_1912.0_7259.0.csv: 698 rows\n",
      "ULHASNAGAR_9999.0_9999.0.csv: 700 rows\n",
      "URAN_1854.0_7255.0.csv: 701 rows\n",
      "VAIBHAVWADI_9999.0_9999.0.csv: 700 rows\n",
      "VASAI_1920.0_7248.0.csv: 699 rows\n",
      "VIKRAMGAD_1947.0_7104.0.csv: 701 rows\n",
      "WADA_1939.0_7308.0.csv: 698 rows\n",
      "ULHASNAGAR_9999.0_9999.0.csv: 700 rows\n",
      "URAN_1854.0_7255.0.csv: 701 rows\n",
      "VAIBHAVWADI_9999.0_9999.0.csv: 700 rows\n",
      "VASAI_1920.0_7248.0.csv: 699 rows\n",
      "VIKRAMGAD_1947.0_7104.0.csv: 701 rows\n",
      "WADA_1939.0_7308.0.csv: 698 rows\n"
     ]
    }
   ],
   "source": [
    "per_station_folder = 'select-stations'\n",
    "output_features_folder = 'output_features'\n",
    "output_folder = 'feature-label-comb'\n",
    "\n",
    "generate_station_wise_outputs(lat_lon_list, per_station_folder, output_features_folder, output_folder)\n",
    "#print length of each file in the output folder\n",
    "for filename in os.listdir(output_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"{filename}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9281c55",
   "metadata": {},
   "source": [
    "## binarizing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312c7c4",
   "metadata": {},
   "source": [
    "- station-wise binarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c3d2dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: subset_18.8833_77.8333.csv\n",
      "Saved: subset_18.5333_73.3167.csv\n",
      "Saved: subset_17.6667_75.9.csv\n",
      "Saved: subset_16.05_73.4667.csv\n",
      "Saved: subset_19.5167_75.9833.csv\n",
      "Saved: subset_17.9833_73.4667.csv\n",
      "Saved: subset_17.8333_73.7.csv\n",
      "Saved: subset_21.75_74.0.csv\n",
      "Saved: subset_20.2_76.0167.csv\n",
      "Saved: subset_21.25_76.0333.csv\n",
      "Saved: subset_16.9833_73.3333.csv\n",
      "Saved: subset_16.8333_73.95.csv\n",
      "Saved: subset_21.5667_74.2167.csv\n",
      "Saved: subset_19.1167_72.85.csv\n",
      "Saved: subset_16.0167_73.7.csv\n",
      "Saved: subset_19.6_76.2167.csv\n",
      "Saved: subset_19.7167_77.15.csv\n",
      "Saved: subset_20.2667_75.7667.csv\n",
      "Saved: subset_18.1333_73.1167.csv\n",
      "Saved: subset_18.8667_73.2833.csv\n",
      "Saved: subset_18.7333_73.1.csv\n",
      "Saved: subset_18.2333_73.2833.csv\n",
      "Saved: subset_19.65_76.3833.csv\n",
      "Saved: subset_18.05_73.0167.csv\n",
      "Saved: subset_20.9333_75.3333.csv\n",
      "Saved: subset_18.9_72.9167.csv\n",
      "Saved: subset_19.7833_71.0667.csv\n",
      "Saved: subset_18.9_72.8167.csv\n",
      "Saved: subset_18.3167_72.9667.csv\n",
      "Saved: subset_19.5667_74.2167.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def binarize_and_add_features(input_folder, lat_lon_list, output_folder='binarized_outputs'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for (filename, lat, lon) in lat_lon_list:\n",
    "        path = os.path.join(input_folder, filename)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File not found: {filename}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if 'Rainfall' not in df.columns or 'Date' not in df.columns:\n",
    "            print(f\"Skipping {filename}: Required columns missing.\")\n",
    "            continue\n",
    "\n",
    "        # Convert Date to datetime to extract month and dayofyear\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])  # drop rows where date parsing failed\n",
    "        df['month'] = df['Date'].dt.month\n",
    "        df['dayofyear'] = df['Date'].dt.dayofyear\n",
    "\n",
    "        # Drop 'Date' and 'Station' columns\n",
    "        df = df.drop(columns=[col for col in ['Date', 'Station'] if col in df.columns])\n",
    "\n",
    "        # Binarize Rainfall using 85th percentile\n",
    "        threshold = df['Rainfall'].quantile(0.85)\n",
    "        df['Rainfall'] = (df['Rainfall'] >= threshold).astype(int)\n",
    "\n",
    "        # Add station coordinates\n",
    "        df['station_lat'] = lat\n",
    "        df['station_lon'] = lon\n",
    "\n",
    "        # Save file\n",
    "        out_filename = f\"subset_{lat}_{lon}.csv\"\n",
    "        df.to_csv(os.path.join(output_folder, out_filename), index=False)\n",
    "        print(f\"Saved: {out_filename}\")\n",
    "input_folder = 'station_outputs'\n",
    "\n",
    "binarize_and_add_features(input_folder, lat_lon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e7cb3",
   "metadata": {},
   "source": [
    "- global binarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ef49f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global 85th percentile rainfall threshold: 65.0000\n",
      "Number of common dates across all files: 615\n",
      "Length of AMBERNATH_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of BHIWANDI_1918.0_7303.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.3_73.05.csv\n",
      "Length of CANACONA_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of CHIPLUN_1732.0_7331.0.csv: 615 rows after filtering common dates\n",
      "Length of CANACONA_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of CHIPLUN_1732.0_7331.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.5333_73.5167.csv\n",
      "Length of COLABA_-_IMD_OBSY_1854.0_7249.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9_72.8167.csv\n",
      "Length of DAHANU_-_IMD_OBSY_1958.0_7243.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.5333_73.5167.csv\n",
      "Length of COLABA_-_IMD_OBSY_1854.0_7249.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9_72.8167.csv\n",
      "Length of DAHANU_-_IMD_OBSY_1958.0_7243.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.9667_72.7167.csv\n",
      "Length of DAPOLI_AGRI_1746.0_7312.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.7667_73.2.csv\n",
      "Length of DEVGAD_1623.0_7321.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.9667_72.7167.csv\n",
      "Length of DAPOLI_AGRI_1746.0_7312.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.7667_73.2.csv\n",
      "Length of DEVGAD_1623.0_7321.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.3833_73.35.csv\n",
      "Length of DODAMARG_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of GAGANBAWADA_1633.0_7350.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.3833_73.35.csv\n",
      "Length of DODAMARG_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of GAGANBAWADA_1633.0_7350.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.55_73.8333.csv\n",
      "Length of GUHAGARH_1728.0_7312.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.4667_73.2.csv\n",
      "Length of HARNAI_IMD_OBSY_1749.0_7306.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.55_73.8333.csv\n",
      "Length of GUHAGARH_1728.0_7312.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.4667_73.2.csv\n",
      "Length of HARNAI_IMD_OBSY_1749.0_7306.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.8167_73.1.csv\n",
      "Length of JAWHAR_1955.0_7314.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.9167_73.2333.csv\n",
      "Length of KALYAN_1915.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.8167_73.1.csv\n",
      "Length of JAWHAR_1955.0_7314.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.9167_73.2333.csv\n",
      "Length of KALYAN_1915.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.25_73.1167.csv\n",
      "Length of KANKAVLI_1616.0_7342.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.2667_73.7.csv\n",
      "Length of KARJAT_AGRI_1855.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.25_73.1167.csv\n",
      "Length of KANKAVLI_1616.0_7342.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.2667_73.7.csv\n",
      "Length of KARJAT_AGRI_1855.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9167_73.3333.csv\n",
      "Length of KHALAPUR_1852.0_7317.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.8667_73.2833.csv\n",
      "Length of KUDAL_1601.0_7342.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9167_73.3333.csv\n",
      "Length of KHALAPUR_1852.0_7317.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.8667_73.2833.csv\n",
      "Length of KUDAL_1601.0_7342.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.0167_73.7.csv\n",
      "Length of LANJA_1652.0_7333.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.8667_73.55.csv\n",
      "Length of MAHAD_1805.0_7325.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.0167_73.7.csv\n",
      "Length of LANJA_1652.0_7333.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.8667_73.55.csv\n",
      "Length of MAHAD_1805.0_7325.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.0833_73.4167.csv\n",
      "Length of MALVAN_1603.0_7328.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.05_73.4667.csv\n",
      "Length of MANDANGAD_1759.0_7315.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.0833_73.4167.csv\n",
      "Length of MALVAN_1603.0_7328.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.05_73.4667.csv\n",
      "Length of MANDANGAD_1759.0_7315.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.9833_73.25.csv\n",
      "Length of MANGAON_1814.0_7317.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.2333_73.2833.csv\n",
      "Length of MATHERAN_1859.0_7317.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9833_73.2833.csv\n",
      "Saved: subset_17.9833_73.25.csv\n",
      "Length of MANGAON_1814.0_7317.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.2333_73.2833.csv\n",
      "Length of MATHERAN_1859.0_7317.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9833_73.2833.csv\n",
      "Length of MHASLA_1808.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.1333_73.1167.csv\n",
      "Length of MOKHEDA_-_FMO_1956.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.9333_73.3333.csv\n",
      "Length of MHASLA_1808.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.1333_73.1167.csv\n",
      "Length of MOKHEDA_-_FMO_1956.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.9333_73.3333.csv\n",
      "Length of MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of MURBAD_1914.0_7324.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.2333_73.4.csv\n",
      "Length of MURUD_1819.0_7258.0.csv: 615 rows after filtering common dates\n",
      "Length of MORMUGAO_-_PMO_IMD_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of MURBAD_1914.0_7324.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.2333_73.4.csv\n",
      "Length of MURUD_1819.0_7258.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.3167_72.9667.csv\n",
      "Length of PALGHAR_AGRI_1945.0_7241.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.75_72.6833.csv\n",
      "Length of PANJIM_-_IMD_OBSY_1529.0_7349.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.3167_72.9667.csv\n",
      "Length of PALGHAR_AGRI_1945.0_7241.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.75_72.6833.csv\n",
      "Length of PANJIM_-_IMD_OBSY_1529.0_7349.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_15.4833_73.8167.csv\n",
      "Length of PANVEL_AGRI_1859.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9833_73.1167.csv\n",
      "Length of PEN_1844.0_7306.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.7333_73.1.csv\n",
      "Saved: subset_15.4833_73.8167.csv\n",
      "Length of PANVEL_AGRI_1859.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9833_73.1167.csv\n",
      "Length of PEN_1844.0_7306.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.7333_73.1.csv\n",
      "Length of PERNEM_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of POLADPUR_1759.0_7328.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.9833_73.4667.csv\n",
      "Length of PURANDAR_SASVAD_1821.0_7402.0.csv: 615 rows after filtering common dates\n",
      "Length of PERNEM_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of POLADPUR_1759.0_7328.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_17.9833_73.4667.csv\n",
      "Length of PURANDAR_SASVAD_1821.0_7402.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.35_74.0333.csv\n",
      "Length of RAJAPUR_1639.0_7331.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.65_73.5167.csv\n",
      "Length of RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.35_74.0333.csv\n",
      "Length of RAJAPUR_1639.0_7331.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.65_73.5167.csv\n",
      "Length of RATNAGIRI_-_IMD_OBSY_1659.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_16.9833_73.3333.csv\n",
      "Length of ROHA_1826.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.4333_73.1167.csv\n",
      "Length of SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Saved: subset_16.9833_73.3333.csv\n",
      "Length of ROHA_1826.0_7307.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.4333_73.1167.csv\n",
      "Length of SANGAMESHWAR_DEVRUKH_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.1167_72.85.csv\n",
      "Length of SAWANTWADI_1554.0_7349.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_15.9_73.8167.csv\n",
      "Length of SANTACRUZ_-_IMD_OBSY_1907.0_7251.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.1167_72.85.csv\n",
      "Length of SAWANTWADI_1554.0_7349.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_15.9_73.8167.csv\n",
      "Length of SHAHAPUR_1927.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.45_73.3333.csv\n",
      "Length of SHRIWARDHAN_1803.0_7301.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.05_73.0167.csv\n",
      "Length of SHAHAPUR_1927.0_7320.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.45_73.3333.csv\n",
      "Length of SHRIWARDHAN_1803.0_7301.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.05_73.0167.csv\n",
      "Length of SUDHAGAD_PALI_1832.0_7319.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.5333_73.3167.csv\n",
      "Length of TALASARI_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of SUDHAGAD_PALI_1832.0_7319.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.5333_73.3167.csv\n",
      "Length of TALASARI_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of TALA_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of THANE_1912.0_7259.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.2_72.9833.csv\n",
      "Length of TALA_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of THANE_1912.0_7259.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.2_72.9833.csv\n",
      "Length of ULHASNAGAR_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of URAN_1854.0_7255.0.csv: 615 rows after filtering common dates\n",
      "Length of ULHASNAGAR_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of URAN_1854.0_7255.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9_72.9167.csv\n",
      "Length of VAIBHAVWADI_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of VASAI_1920.0_7248.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_18.9_72.9167.csv\n",
      "Length of VAIBHAVWADI_9999.0_9999.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_100.65_100.65.csv\n",
      "Length of VASAI_1920.0_7248.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.3333_72.8.csv\n",
      "Length of VIKRAMGAD_1947.0_7104.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.7833_71.0667.csv\n",
      "Length of WADA_1939.0_7308.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.65_73.1333.csv\n",
      "Saved: subset_19.3333_72.8.csv\n",
      "Length of VIKRAMGAD_1947.0_7104.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.7833_71.0667.csv\n",
      "Length of WADA_1939.0_7308.0.csv: 615 rows after filtering common dates\n",
      "Saved: subset_19.65_73.1333.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def binarize_global_threshold(input_folder, lat_lon_list, output_folder='overall_binarized_outputs'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 1: Gather all rainfall values and collect dates from all stations\n",
    "    all_rainfall = []\n",
    "    date_sets = []\n",
    "    for (filename, _, _) in lat_lon_list:\n",
    "        path = os.path.join(input_folder, filename)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            if 'Rainfall' in df.columns and 'Date' in df.columns:\n",
    "                all_rainfall.extend(df['Rainfall'].tolist())\n",
    "                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                date_sets.append(set(df['Date'].dropna().dt.strftime('%Y-%m-%d')))\n",
    "\n",
    "    # Find intersection of all date sets (dates present in every file)\n",
    "    if date_sets:\n",
    "        common_dates = set.intersection(*date_sets)\n",
    "    else:\n",
    "        common_dates = set()\n",
    "\n",
    "    # Step 2: Compute global 85th percentile threshold\n",
    "    global_threshold = pd.Series(all_rainfall).quantile(0.85)\n",
    "    print(f\"Global 85th percentile rainfall threshold: {global_threshold:.4f}\")\n",
    "    print(f\"Number of common dates across all files: {len(common_dates)}\")\n",
    "\n",
    "    # Step 3: Apply binarization and feature addition per station\n",
    "    for (filename, lat, lon) in lat_lon_list:\n",
    "        path = os.path.join(input_folder, filename)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"File not found: {filename}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        if 'Rainfall' not in df.columns or 'Date' not in df.columns:\n",
    "            print(f\"Skipping {filename}: Required columns missing.\")\n",
    "            continue\n",
    "\n",
    "        # Parse date and extract features\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        # Keep only rows with Date in common_dates\n",
    "        df = df[df['Date'].dt.strftime('%Y-%m-%d').isin(common_dates)]\n",
    "        df['month'] = df['Date'].dt.month\n",
    "        df['dayofyear'] = df['Date'].dt.dayofyear\n",
    "\n",
    "        # Drop Date and Station columns if present\n",
    "        df = df.drop(columns=[col for col in ['Date', 'Station'] if col in df.columns])\n",
    "\n",
    "        # Binarize using global threshold\n",
    "        df['Rainfall'] = (df['Rainfall'] >= global_threshold).astype(int)\n",
    "\n",
    "        # Add station lat/lon\n",
    "        df['station_lat'] = lat\n",
    "        df['station_lon'] = lon\n",
    "\n",
    "        # Replace any missing values in the DataFrame with 0\n",
    "        df.fillna(0, inplace=True)\n",
    "\n",
    "        # Print the length of the DataFrame\n",
    "        print(f\"Length of {filename}: {len(df)} rows after filtering common dates\")\n",
    "\n",
    "        # Save the processed file\n",
    "        out_filename = f\"subset_{lat}_{lon}.csv\"\n",
    "        df.to_csv(os.path.join(output_folder, out_filename), index=False)\n",
    "        print(f\"Saved: {out_filename}\")\n",
    "input_folder = 'feature-label-comb'\n",
    "\n",
    "binarize_global_threshold(input_folder, lat_lon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa9f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
